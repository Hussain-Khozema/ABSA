{"cells":[{"cell_type":"markdown","metadata":{"id":"ImdaUDI3782w"},"source":["## Mount Drive"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWP748mQ8A17","executionInfo":{"status":"ok","timestamp":1647752651953,"user_tz":-480,"elapsed":13409,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"1dfbf59d-b8f7-4472-b899-bf5969d90309"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# %cd /content/drive/My Drive/Ntu/nndl/CZ4042 Final Project\n","\n","%cd /content/drive/My Drive/"]},{"cell_type":"markdown","metadata":{"id":"GHPIs_bEfR2a"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17512,"status":"ok","timestamp":1646805410322,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"},"user_tz":-480},"id":"USqqTyfxA5H-","outputId":"6a4a465e-c572-45e6-d65d-d804fd5dd2d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 20.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 17.9 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 26.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 41.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybn5lQrbM7jw"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","import torch\n","from pathlib import Path\n","import os\n","import re\n","\n","import pandas as pd\n","import xml.etree.ElementTree as ET"]},{"cell_type":"markdown","metadata":{"id":"zTgBQp_TERDx"},"source":["## Save Data files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ErhzywyYuudl"},"outputs":[],"source":["data_dir = 'sameval2014/data'\n","semeval_sentiments = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","semeval_label2id = {\"positive\": 0, \"neutral\": 1, \"negative\": 2, \"conflict\": 3, \"none\": 4}\n","\n","for aspect in aspects:\n","  path = data_dir + '/' + aspect\n","\n","  os.makedirs(path, exist_ok=True)\n","\n","os.makedirs(data_dir + '/predictions', exist_ok = True)\n","\n","def aspects_data(xml_file_path, aspect):\n","  if aspect == 'anecdotes':\n","    tmp_aspect = 'anecdotes/miscellaneous'\n","  else:\n","    tmp_aspect = aspect\n","  output = []\n","  with open(xml_file_path, \"r\") as f:\n","    tree = ET.parse(f)\n","    root = tree.getroot()\n","    for sentence in root:\n","        id = sentence.attrib[\"id\"]\n","        text = sentence.find(\"text\").text\n","        label = \"none\"\n","        for opinion in sentence.find(\"aspectCategories\"):\n","          if opinion.attrib[\"category\"] == tmp_aspect:\n","            label = opinion.attrib[\"polarity\"]\n","        data = [id, text, aspect, semeval_label2id[label], label]\n","        output.append(data)\n","  return output"]},{"cell_type":"markdown","metadata":{"id":"XmG84sagEZmK"},"source":["### Save Train Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-X4u-TgtEchp"},"outputs":[],"source":["train_path = data_dir + '/Restaurants_Train.xml'\n","for aspect in aspects:\n","  data = aspects_data(train_path, aspect)\n","  # data = sorted(data, key=lambda el: el[0])\n","  df = pd.DataFrame(data, columns = [\"id\", \"text\", \"aspect\", \"label_id\", \"label\"])\n","  df.to_csv(\"{}/{}/train.csv\".format(data_dir, aspect), sep='\\t', index=False)"]},{"cell_type":"markdown","metadata":{"id":"cx4yZZPWEdnC"},"source":["### Save Test Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFOfHadAEgzb"},"outputs":[],"source":["test_path = data_dir + '/Restaurants_Test_Gold.xml'\n","for aspect in aspects:\n","  data = aspects_data(test_path, aspect)\n","  # data = sorted(data, key=lambda el: el[0])\n","  df = pd.DataFrame(data, columns = [\"id\", \"text\", \"aspect\", \"label_id\", \"label\"])\n","  df.to_csv(\"{}/{}/test.csv\".format(data_dir, aspect), sep='\\t', index=False)"]},{"cell_type":"markdown","metadata":{"id":"SmGjjxUZEiGZ"},"source":["## Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"ird-ygXdEnGg"},"source":["### Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bT2NxYiCYKr"},"outputs":[],"source":["def get_train_data(data_dir, aspect):\n","  train_df = pd.read_csv(\"{}/{}/train.csv\".format(data_dir, aspect),header=0, sep=\"\\t\")\n","\n","  train_sentences = train_df['text'].tolist()\n","  train_labels = train_df['label_id'].tolist()\n","  return train_sentences, train_labels\n","\n","def get_test_data(data_dir, aspect):\n","  test_df = pd.read_csv(\"{}/{}/test.csv\".format(data_dir, aspect),header=0, sep=\"\\t\")\n","\n","  test_sentences = test_df['text'].tolist()\n","  test_labels = test_df['label_id'].tolist()\n","  return test_sentences, test_labels"]},{"cell_type":"code","source":["df = pd.read_csv(\"{}/{}/train.csv\".format(data_dir, 'food'),header=0, sep=\"\\t\")\n","print(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UzRcNMRDXf5I","executionInfo":{"status":"ok","timestamp":1647612724737,"user_tz":-480,"elapsed":425,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"f54d294b-931f-4198-faed-e374b9525cc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["        id                                               text aspect  \\\n","0     3121               But the staff was so horrible to us.   food   \n","1     2777  To be completely fair, the only redeeming fact...   food   \n","2     1634  The food is uniformly exceptional, with a very...   food   \n","3     2534  Where Gabriela personaly greets you and recomm...   food   \n","4      583  For those that go once and don't enjoy it, all...   food   \n","...    ...                                                ...    ...   \n","3039  1063                     But that is highly forgivable.   food   \n","3040   777  From the appetizers we ate, the dim sum and ot...   food   \n","3041   875  When we arrived at 6:00 PM, the restaurant was...   food   \n","3042   671  Each table has a pot of boiling water sunken i...   food   \n","3043   617          I am going to the mid town location next.   food   \n","\n","      label_id     label  \n","0            4      none  \n","1            0  positive  \n","2            0  positive  \n","3            4      none  \n","4            4      none  \n","...        ...       ...  \n","3039         4      none  \n","3040         0  positive  \n","3041         4      none  \n","3042         1   neutral  \n","3043         4      none  \n","\n","[3044 rows x 5 columns]\n"]}]},{"cell_type":"code","source":["for aspect in aspects:\n","  train_sentences, train_labels = get_train_data(data_dir, aspect)\n","  test_sentences, test_labels = get_test_data(data_dir, aspect)"],"metadata":{"id":"ETmrU_EkrxBj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEJuni3gFCAr"},"outputs":[],"source":["class Semeval_Data(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"markdown","source":["## Data Analysis"],"metadata":{"id":"LvRbJu-lpcMo"}},{"cell_type":"markdown","source":["### Number of sentences in each aspect"],"metadata":{"id":"15AuNzXZpgfd"}},{"cell_type":"code","source":["label_count = [[],[]]\n","for aspect in aspects:\n","  train_sentences, train_labels = get_train_data(data_dir, aspect)\n","  test_sentences, test_labels = get_test_data(data_dir, aspect)\n","\n","  train_count = 0\n","  test_count = 0\n","  for label in train_labels:\n","    if label != 4:\n","      train_count = train_count+1\n","  \n","  for label in test_labels:\n","    if label != 4:\n","      test_count = test_count+1\n","\n","  label_count[0].append(train_count)\n","  label_count[1].append(test_count)\n","  print(\"Number of train sentences for aspect {}: {}\".format(aspect, train_count))\n","  print(\"Number of test sentences for aspect {}: {}\".format(aspect, test_count))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvuGLObSpnw2","executionInfo":{"status":"ok","timestamp":1647098647247,"user_tz":-480,"elapsed":312,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"7f2607bf-fc36-4296-9232-1b0e69fa1928"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of train sentences for aspect price: 319\n","Number of test sentences for aspect price: 83\n","Number of train sentences for aspect anecdotes: 1131\n","Number of test sentences for aspect anecdotes: 234\n","Number of train sentences for aspect food: 1233\n","Number of test sentences for aspect food: 418\n","Number of train sentences for aspect ambience: 432\n","Number of test sentences for aspect ambience: 118\n","Number of train sentences for aspect service: 597\n","Number of test sentences for aspect service: 172\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n"," \n","# set width of bar\n","barWidth = 0.25\n","fig = plt.subplots(figsize =(12, 8))\n"," \n","# set height of bar\n","Train = label_count[0]\n","Test = label_count[1]\n"," \n","# Set position of bar on X axis\n","br1 = np.arange(len(Train))\n","br2 = [x + barWidth for x in br1]\n"," \n","# Make the plot\n","plt.bar(br1, Train, color ='b', width = barWidth,\n","        edgecolor ='grey', label ='Train')\n","plt.bar(br2, Test, color ='g', width = barWidth,\n","        edgecolor ='grey', label ='test')\n"," \n","# Adding Xticks\n","plt.xlabel('Aspects', fontsize = 10)\n","plt.ylabel('Count', fontsize = 10)\n","plt.xticks([r + barWidth for r in range(len(Train))], aspects)\n"," \n","plt.legend()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"id":"-xBFTLwxuLI6","executionInfo":{"status":"ok","timestamp":1647098648182,"user_tz":-480,"elapsed":632,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"3c77db90-5fe6-4a5b-897a-56baeca26d17"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x576 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAtoAAAHgCAYAAACb58plAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7xedX0n+s/XJO7IRVCIFAk21KKjoy3aeKWnx0tV0BkvM+K9omUOx0tBxoIGq2XHc3omHj2KdiozVBEdr9RLxcsoiFBovQZMFQRLihWCFlKqEfAQAv3NH88K3cTs3Ni//WTvvN+v1/Paa/3Wb6313TwPz/PJb/+etaq1FgAAYGbda9wFAADAfCRoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAcLx11ADwceeGBbtmzZuMsAAGCeu/TSS/+ptbZka9vmZdBetmxZVq9ePe4yAACY56rqR9NtM3UEAAA6ELQBAKADQRsAADqYl3O0AQDob9OmTVm3bl1uu+22cZfS3eLFi7N06dIsWrRoh/cRtAEA2CXr1q3Lvvvum2XLlqWqxl1ON6213HTTTVm3bl0OO+ywHd7P1BEAAHbJbbfdlgMOOGBeh+wkqaoccMABOz1yL2gDALDL5nvI3mxXfk9BGwCAOemmm27KEUcckSOOOCK/8iu/kkMOOeSu9dtvv32b+65evTonnnhi1/rM0QYAYEasWnV6Nm7cMGPHm5jYLytWnDTt9gMOOCBr1qxJkkxOTmafffbJySeffNf2O+64IwsXbj3uLl++PMuXL5+xWrdG0AYAYEZs3Lghk5OnzdjxJidX7vQ+r3jFK7J48eJ85zvfyZFHHpkXvehFed3rXpfbbrst97nPffKBD3wgD33oQ3PRRRflHe94Rz7/+c9ncnIy1157ba655ppce+21Oemkk2ZktFvQBgBgXlm3bl2+9rWvZcGCBfn5z3+eSy65JAsXLsxXvvKVvOlNb8qnPvWpX9rnqquuyoUXXpibb745D33oQ/PqV796py7ltzWCNgAA88oxxxyTBQsWJEk2bNiQY489NldffXWqKps2bdrqPs961rMyMTGRiYmJPOABD8gNN9yQpUuX3qM6fBkSAIB5Ze+9975r+S1veUue/OQn5/LLL8/nPve5aS/RNzExcdfyggULcscdd9zjOgRtAADmrQ0bNuSQQw5Jkpx99tmzem5BGwCAeesNb3hDTj311DzqUY+akVHqnVGttVk94WxYvnx5W7169bjLAACY16688so87GEPu2t9ti/vN9u2/H2TpKouba1t9TqBvgwJAMCM2J1C8e7A1BEAAOhA0AYAgA5MHQEYs5me07irdre5kABznaANMGYzfcviXbUrtzoGYHqmjgAAQAeCNgAAc9LPfvazvPe9792lfU8//fT84he/mOGK7s7UEQAAZsSqd6zKxls3ztjxJvaeyIqTV0y7fXPQfs1rXrPTxz799NPzspe9LHvttdc9KXGbBG0AAGbExls3ZjKTM3a8yVu3fawVK1bk7//+73PEEUfkaU97Wh7wgAfknHPOycaNG/O85z0vK1euzK233poXvOAFWbduXe6888685S1vyQ033JAf//jHefKTn5wDDzwwF1544YzVPJWgDQDAnLRq1apcfvnlWbNmTc4777x88pOfzLe+9a201vLsZz87F198cdavX58HPvCB+cIXvpAk2bBhQ/bbb7+8853vzIUXXpgDDzywW33d5mhX1VlVdWNVXT6l7e1VdVVVfbeqPlNV+0/ZdmpVra2qH1TVM6a0HzW0ra2q6f92AADAHuu8887Leeedl0c96lF59KMfnauuuipXX311HvnIR+b888/PG9/4xlxyySXZb7/9Zq2mnl+GPDvJUVu0nZ/kEa2130jyd0lOTZKqeniSFyX5t8M+762qBVW1IMmfJTk6ycOTvHjoCwAAd2mt5dRTT82aNWuyZs2arF27Nscdd1we8pCH5LLLLssjH/nIvPnNb85b3/rWWaupW9BurV2c5J+3aDuvtXbHsPqNJEuH5eck+XhrbWNr7YdJ1iZ57PBY21q7prV2e5KPD30BANjD7bvvvrn55puTJM94xjNy1lln5ZZbbkmSXH/99bnxxhvz4x//OHvttVde9rKX5ZRTTslll132S/v2Ms452r+f5BPD8iEZBe/N1g1tSXLdFu2P618aAAC7uwMOOCBHHnlkHvGIR+Too4/OS17ykjzhCU9Ikuyzzz758Ic/nLVr1+aUU07Jve51ryxatChnnHFGkuT444/PUUcdlQc+8IHz68uQVfVHSe5I8pEZPObxSY5Pkgc96EEzdVgAAHbQxN4T271SyM4eb3s++tGP3m39da973d3WH/zgB+cZz3hGtnTCCSfkhBNOuGcFbsesB+2qekWSf5fkqa21NjRfn+TQKd2WDm3ZRvvdtNbOTHJmkixfvrxtrQ8AAP1s65rXe6JZvTNkVR2V5A1Jnt1am3ornnOTvKiqJqrqsCSHJ/lWkm8nObyqDquqe2f0hclzZ7NmAADYFd1GtKvqY0melOTAqlqX5LSMrjIykeT8qkqSb7TWXtVau6Kqzkny/YymlLy2tXbncJw/SPLlJAuSnNVau6JXzQAAMFO6Be3W2ou30vz+bfT/kyR/spX2Lyb54gyWBgDADGmtZRhAndf+dcbzjpvVqSMAAMwfixcvzk033bRLIXQuaa3lpptuyuLFi3dqP7dgBwBglyxdujTr1q3L+vXrx11Kd4sXL87SpUu333EKQRsAgF2yaNGiHHbYYeMuY7dl6ggAAHQgaAMAQAeCNgAAdCBoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAeCNgAAdCBoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAeCNgAAdLBw3AXAnmLVqtOzceOGcZeRiYn9smLFSeMuAwDmPUEbZsnGjRsyOXnauMvI5OTKcZcAAHsEU0cAAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADroFrSr6qyqurGqLp/Sdv+qOr+qrh5+3m9or6p6T1WtrarvVtWjp+xz7ND/6qo6tle9AAAwk3qOaJ+d5Kgt2lYkuaC1dniSC4b1JDk6yeHD4/gkZySjYJ7ktCSPS/LYJKdtDucAALA76xa0W2sXJ/nnLZqfk+SDw/IHkzx3SvuH2sg3kuxfVQcneUaS81tr/9xa+2mS8/PL4R0AAHY7sz1H+6DW2k+G5X9MctCwfEiS66b0Wze0TdcOAAC7tbF9GbK11pK0mTpeVR1fVauravX69etn6rAAALBLZjto3zBMCcnw88ah/fokh07pt3Rom679l7TWzmytLW+tLV+yZMmMFw4AADtjtoP2uUk2Xznk2CSfndL+8uHqI49PsmGYYvLlJE+vqvsNX4J8+tAGAAC7tYW9DlxVH0vypCQHVtW6jK4esirJOVV1XJIfJXnB0P2LSZ6ZZG2SXyR5ZZK01v65qv6vJN8e+r21tbblFywBAGC30y1ot9ZePM2mp26lb0vy2mmOc1aSs2awNAAA6M6dIQEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6GEvQrqr/XFVXVNXlVfWxqlpcVYdV1Teram1VfaKq7j30nRjW1w7bl42jZgAA2BmzHrSr6pAkJyZZ3lp7RJIFSV6U5G1J3tVa+/UkP01y3LDLcUl+OrS/a+gHAAC7tXFNHVmY5D5VtTDJXkl+kuQpST45bP9gkucOy88Z1jNsf2pV1SzWCgAAO23Wg3Zr7fok70hybUYBe0OSS5P8rLV2x9BtXZJDhuVDklw37HvH0P+A2awZAAB21jimjtwvo1Hqw5I8MMneSY6ageMeX1Wrq2r1+vXr7+nhAADgHhnH1JHfTfLD1tr61tqmJJ9OcmSS/YepJEmyNMn1w/L1SQ5NkmH7fklu2vKgrbUzW2vLW2vLlyxZ0vt3AACAbRpH0L42yeOraq9hrvVTk3w/yYVJnj/0OTbJZ4flc4f1DNu/2lprs1gvAADstHHM0f5mRl9qvCzJ94YazkzyxiSvr6q1Gc3Bfv+wy/uTHDC0vz7JitmuGQAAdtbC7XeZea2105KctkXzNUkeu5W+tyU5ZjbqAgCAmeLOkAAA0IGgDQAAHQjaAADQgaANAAAdCNoAANCBoA0AAB0I2gAA0IGgDQAAHQjaAADQgaANAAAdCNoAANCBoA0AAB0I2gAA0IGgDQAAHQjaAADQgaANAAAdCNoAANCBoA0AAB0I2gAA0IGgDQAAHQjaAADQgaANAAAdCNoAANCBoA0AAB0I2gAA0IGgDQAAHQjaAADQgaANAAAdCNoAANCBoA0AAB0I2gAA0IGgDQAAHQjaAADQwQ4F7ao6ckfaAACAkR0d0f7THWwDAACSLNzWxqp6QpInJllSVa+fsum+SRb0LAwAAOaybQbtJPdOss/Qb98p7T9P8vxeRQEAwFy3zaDdWvurJH9VVWe31n40SzUBAMCct70R7c0mqurMJMum7tNae0qPogAAYK7b0aD9F0n+W5L3JbmzXzkAADA/7GjQvqO1dkbXSgAAYB7Z0cv7fa6qXlNVB1fV/Tc/ulYGAABz2I6OaB87/DxlSltL8mszWw4AAMwPOxS0W2uH9S4EAADmkx0K2lX18q21t9Y+NLPlAADA/LCjU0ceM2V5cZKnJrksiaANAABbsaNTR06Yul5V+yf5eJeKAABgHtjRq45s6dYk5m0DAMA0dnSO9ucyuspIkixI8rAk5/QqCgAA5rodnaP9jinLdyT5UWttXYd6AABgXtihqSOttb9KclWSfZPcL8ntPYsCAIC5boeCdlW9IMm3khyT5AVJvllVz+9ZGAAAzGU7OnXkj5I8prV2Y5JU1ZIkX0nyyV6FAQDAXLajVx251+aQPbhpJ/YFAIA9zo6OaH+pqr6c5GPD+guTfLFPSQAAMPdtM2hX1a8nOai1dkpV/Yckvz1s+nqSj/QuDgAA5qrtjWifnuTUJGmtfTrJp5Okqh45bPv3XasDAIA5anvzrA9qrX1vy8ahbVmXigAAYB7YXtDefxvb7jOThQAAwHyyvaC9uqr+jy0bq+o/Jbl0V09aVftX1Ser6qqqurKqnlBV96+q86vq6uHn/Ya+VVXvqaq1VfXdqnr0rp4XAABmy/bmaJ+U5DNV9dL8a7BenuTeSZ53D8777iRfaq09v6runWSvJG9KckFrbVVVrUiyIskbkxyd5PDh8bgkZww/AQBgt7XNoN1auyHJE6vqyUkeMTR/obX21V09YVXtl+R3krxiOMftSW6vquckedLQ7YNJLsooaD8nyYdaay3JN4bR8INbaz/Z1RoAAKC3HbqOdmvtwiQXztA5D0uyPskHquo3Mxopf11GX7zcHJ7/MclBw/IhSa6bsv+6oe1uQbuqjk9yfJI86EEPmqFSAQBg1+zoDWtm+pyPTnJCa+2bVfXujKaJ3KW11qqq7cxBW2tnJjkzSZYvX75T+wIAzDWrVp2ejRs3jLuMTEzslxUrThp3GbulcQTtdUnWtda+Oax/MqOgfcPmKSFVdXCSzbd8vz7JoVP2Xzq0AQDssTZu3JDJydPGXUYmJ1eOu4Td1vauOjLjWmv/mOS6qnro0PTUJN9Pcm6SY4e2Y5N8dlg+N8nLh6uPPD7JBvOzAQDY3Y1jRDtJTkjykeGKI9ckeWVGof+cqjouyY+SvGDo+8Ukz0yyNskvhr4AALBbG0vQbq2tyegygVt66lb6tiSv7V4UAADMoFmfOgIAAHsCQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoIOF4y4AAJjeqlWnZ+PGDeMuIxMT+2XFipPGXQbMKYI2AOzGNm7ckMnJ08ZdRiYnV467BJhzTB0BAIAOBG0AAOjA1BEAkiSbsikrV45/esDE3hNZcfKKcZcBcI8J2gAkSRZlUSYzOe4yMnnr+GsAmAmmjgAAQAdjC9pVtaCqvlNVnx/WD6uqb1bV2qr6RFXde2ifGNbXDtuXjatmAADYUeMc0X5dkiunrL8tybtaa7+e5KdJjhvaj0vy06H9XUM/AADYrY0laFfV0iTPSvK+Yb2SPCXJJ4cuH0zy3GH5OcN6hu1PHfoDAMBua1wj2qcneUOSfxnWD0jys9baHcP6uiSHDMuHJLkuSYbtG4b+d1NVx1fV6qpavX79+p61AwDAds160K6qf5fkxtbapTN53Nbama215a215UuWLJnJQwMAwE4bx+X9jkzy7Kp6ZpLFSe6b5N1J9q+qhcOo9dIk1w/9r09yaJJ1VbUwyX5Jbpr9sgEAYMfN+oh2a+3U1trS1tqyJC9K8tXW2kuTXJjk+UO3Y5N8dlg+d1jPsP2rrbU2iyUDAMBO252uo/3GJK+vqrUZzcF+/9D+/iQHDO2vT+J2YQAA7PbGemfI1tpFSS4alq9J8tit9LktyTGzWhgAANxDu9OINgAAzBuCNgAAdCBoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAeCNgAAdCBoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAeCNgAAdCBoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAeCNgAAdCBoAwBAB4I2AAB0sHDcBcw3q1adno0bN4y7jExM7JcVK04adxkAAHssQXuGbdy4IZOTp427jExOrhx3CQAAezRTRwAAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOlg47gKA2bUpm7Jy5cpxl5GJvSey4uQV4y4DALoRtGEPsyiLMpnJcZeRyVvHXwMA9GTqCAAAdCBoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAezHrSr6tCqurCqvl9VV1TV64b2+1fV+VV19fDzfkN7VdV7qmptVX23qh492zUDAMDOGseI9h1J/rC19vAkj0/y2qp6eJIVSS5orR2e5IJhPUmOTnL48Dg+yRmzXzIAAOycWQ/arbWftNYuG5ZvTnJlkkOSPCfJB4duH0zy3GH5OUk+1Ea+kWT/qjp4lssGAICdMtY52lW1LMmjknwzyUGttZ8Mm/4xyUHD8iFJrpuy27qhDQAAdltjC9pVtU+STyU5qbX286nbWmstSdvJ4x1fVauravX69etnsFIAANh5YwnaVbUoo5D9kdbap4fmGzZPCRl+3ji0X5/k0Cm7Lx3a7qa1dmZrbXlrbfmSJUv6FQ8AADtgHFcdqSTvT3Jla+2dUzadm+TYYfnYJJ+d0v7y4eojj0+yYcoUEwAA2C0tHMM5j0zye0m+V1VrhrY3JVmV5JyqOi7Jj5K8YNj2xSTPTLI2yS+SvHJ2ywUAgJ0360G7tfbXSWqazU/dSv+W5LVdiwIAgBnmzpAAANCBoA0AAB0I2gAA0IGgDQAAHYzjqiMAAMwTm7IpK1euHHcZmdh7IitOXjHuMu5G0AYAYJctyqJMZnLcZWTy1vHXsCVTRwAAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoANBGwAAOhC0AQCgA0EbAAA6WDjuAgCA3d+mbMrKlSvHWsPE3hNZcfKKsdYAO0PQBgC2a1EWZTKTY61h8tbxnh92lqkjAADQgaANAAAdCNoAANCBoA0AAB0I2gAA0IGgDQAAHQjaAADQgaANAAAdCNoAANCBoA0AAB24Bfs8tSmbsnLlynGXkYm9J7Li5BXjLgMAYNYJ2vPUoizKZCbHXUYmbx1/DQAA42DqCAAAdCBoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAeCNgAAdCBoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAeCNgAAdCBoAwBAB4I2AAB0IGgDAEAHgjYAAHQgaAMAQAeCNgAAdCBoAwBAB4I2AAB0IGgDAEAHcyZoV9VRVfWDqlpbVSvGXQ8AAGzLnAjaVbUgyZ8lOTrJw5O8uKoePt6qAABgenMiaCd5bJK1rbVrWmu3J/l4kueMuSYAAJjWXAnahyS5bsr6uqENAAB2S9VaG3cN21VVz09yVGvtPw3rv5fkca21P5jS5/gkxw+rD03yg1kvdP45MMk/jbsIdkteG0zHa4PpeG2wLXP59fGrrbUlW9uwcLYr2UXXJzl0yvrSoe0urbUzk5w5m0XNd1W1urW2fNx1sPvx2mA6XhtMx2uDbZmvr4+5MnXk20kOr6rDqureSV6U5Nwx1wQAANOaEyParbU7quoPknw5yYIkZ7XWrhhzWQAAMK05EbSTpLX2xSRfHHcdexhTcZiO1wbT8dpgOl4bbMu8fH3MiS9DAgDAXDNX5mgDAMCcImjzS6rqrVX1u+Oug91fVb2iqv7rdvo8151c576qOrGqrqyqj9zD4/xDVR04U3Wx+9jW+0FVfW2262Humk85ZM7M0WZ2VNWC1tofj7sO5pXnJvl8ku+PuxDukdck+d3W2rpxF8Lc01p74rhrYPdSVQtba3dsbdt8yiFGtPcgVbWsqq6qqo8MI1OfrKq9hhGmt1XVZUmOqaqzh5sEpaoeU1Vfq6q/rapvVdW+VbWgqt5eVd+uqu9W1f855l+NLVTVX1bVpVV1xXAzp1TVLVX1J8Nz+Y2qOmhoX1JVnxqez29X1ZFD+z5V9YGq+t7wPP/Hof2VVfV3VfWtJEdOOeeyqvrq0PeCqnpQVT0xybOTvL2q1lTVg4fHl4b6LqmqfzPsf0xVXT7Ud/Es/ydjG6rqvyX5tST/s6r+cHh9fXd4Hf3G0Of+07QfUFXnDa/F9yWpMf4q7IBtvH+8fWj7SlU9tqouqqprqurZU3Y/dGi/uqpOm3LMW6YsnzLl82Pl0LZs+Fz68+Ec51XVfYZtvz6c82+r6rKqevB0x2H2VdXeVfWF4fm5vKpeWFW/VVV/NbyOvlxVBw99L6qq06tqdZI/qqofVdW9phznuqpaNK9ySGvNYw95JFmWpCU5clg/K8nJSf4hyRum9Ds7yfOT3DvJNUkeM7TfN6O/ghyf5M1D20SS1UkOG/fv53G35/r+w8/7JLk8yQHDc//vh/b/d8pz+NEkvz0sPyjJlcPy25KcPuWY90tycJJrkywZXh9/k+S/Dts/l+TYYfn3k/zl1NfTlONckOTwYflxSb46LH8vySHD8v7j/m/o8UuvqX/I6M5tf5rktKHtKUnWDMvTtb8nyR8Py88aXocHjvv38djmcz3d+8fRQ/tnkpyXZFGS35zyXL8iyU+G/pv3XT5su2X4+fSMri5RGQ32fT7J7wyfT3ckOWLod06Slw3L30zyvGF5cZK9pjvOuP/b7YmPJP8xyZ9PWd8vydeSLBnWX5jRZZmT5KIk753S97NJnjyl3/uG5bMzT3KIqSN7nutaa38zLH84yYnD8ie20vehSX7SWvt2krTWfp4kVfX0JL+x+V+bGf1PdXiSH3armp11YlU9b1g+NKPn5/aMPoyS5NIkTxuWfzfJw6vuGmi8b1XtM7S/aHNja+2nVfXcJBe11tYnSVV9IslDhi5PSPIfhuX/kVGYv5vhuE9M8hdTzjcx/PybJGdX1TlJPr0LvzOz47cz+mBNa+2rw4j1fbfR/jsZXhettS9U1U/HVDc7brr3jy8Nbd9LsrG1tqmqvpdRSN7s/NbaTUlSVZ/O6HWxesr2pw+P7wzr+wzHvzbJD1tra4b2S5Msq6p9M/oH+GeSpLV223Ds6Y7jr2Gz73tJ/r+qeltGnzE/TfKIJOcP7/MLMvoH2Gaf2GL5hUkuzOjz5r1bHHvO5xBBe8+z5fUcN6/fuhPHqCQntNa+PDMlMZOq6kkZheQntNZ+UVUXZTQKtKkN//xPcmf+9f//eyV5/OYPsCnH6VHevZL8rLV2xJYbWmuvqqrHZTTqeWlV/dbmD2xgduzg+8e/JNmYJK21f6mqqVlius+Yu06R5L+01v77FuddtvmYgzszGhWfttStHYfZ11r7u6p6dJJnJvm/k3w1yRWttSdMs8vUvK3DK0kAAATGSURBVHFukv+nqu6f5LeGfXfEnMkh5mjveR5UVZtf/C9J8tfb6PuDJAdX1WOSZJgXtTCjO3S+uqoWDe0Pqaq9exbNTtkvyU+HD8l/k+Tx2+l/XpITNq9U1eYQfH6S105pv19Gf8L934fRykVJjplynK/lX0fAX5rkkmH55iT7JneNRvywqo4ZjllV9ZvD8oNba99soy/BrM9oJI3dzyUZPb+bQ9k/Dc/rdO0XZ/Rek6o6OqMpSOy+dvb9Y0tPq9F8/ftk9EXov9li+5eT/P7w161U1SFV9YDpDtZauznJuuGvaamqiaraa2ePQz9V9cAkv2itfTjJ2zOaErhkc9YY5lz/263t21q7Jcm3k7w7yedba3du0WXO5xAj2nueHyR5bVWdldFVIM7IlJA1VWvt9qp6YZI/Hd40//+MRjrel9GfCi+r0bDn+ozeUNk9fCnJq6rqyoye729sp/+JSf6sqr6b0XvCxUleldHIxJ9V1eUZjS6tbK19uqomk3w9yc+SrJlynBOSfKCqTsnoNfHKof3jSf68qk7MaM7dS5OcUVVvzmiO58eT/G1GX5g8PKORiguGNnY/k0nOGl4vv0hy7HbaVyb5WFVdkdE/xq6d1WrZWTv7/rGlbyX5VJKlST7cWps6bSSttfOq6mFJvj781eyWJC/L6D1mOr+X5L9X1VuTbEpyzDaOc+NO1ss998iM3r//JaPn59UZzbd/T1Xtl9HnyulJrphm/08k+YskT9pyw3zIIe4MuQcZ/jT3+dbaI8ZcCgDAvGfqCAAAdGBEGwAAOjCiDQAAHQjaAADQgaANAAAdCNoA80BVPbeq2nDt497n2r+qXtP7PABznaANMD+8OKMbUL14Fs61fxJBG2A7BG2AOW64O95vJzkuw905q+rgqrq4qtZU1eVV9b8N7bdU1buq6oqquqCqlgztD66qL1XVpVV1yeaR8ao6qKo+U1V/OzyemGRVkgcPx377dOcC2NO5vB/AHFdVL03ylNbacVX1tYzu0vmkJItba39SVQuS7NVau7mqWpKXtdY+UlV/nOQBrbU/qKoLkryqtXZ1VT0uyX9prT2lqj6R5OuttdOH4+yT0W3U77r5VVX94dbONdv/HQB2N27BDjD3vTjJu4fljw/r52Z0S/RFSf6ytbZm2P4vGd3yOEk+nOTTw4j4E5P8xXA76ySZGH4+JcnLk6S1dmeSDVV1vy3O/+1pzgWwRxO0Aeawqrp/RmH4kcNo9YIkLckpSX4nybOSnF1V72ytfWgrh2gZTSP8WWvtiF2pobV2cVXtyLkA9ijmaAPMbc9P8j9aa7/aWlvWWjs0yQ8zCtk3tNb+PMn7kjx66H+vYZ8keUmSv26t/TzJD6vqmCSpkd8c+lyQ5NVD+4Kq2i/JzUn23VxAVf3qNOcC2KOZow0wh1XVhUne1lr70pS2E5P85yS3JtmU5JYkL2+t/bCqbklyZpKnJ7kxyQtba+ur6rAkZyQ5OMmiJB9vrb21qg4a+v9akjuTvLq19vWq+miS30jyP5NcntEI+t3ONQu/PsBuTdAG2INU1S2ttX3GXQfAnsDUEQAA6MCINgAAdGBEGwAAOhC0AQCgA0EbAAA6ELQBAKADQRsAADoQtAEAoIP/BUWvR7ECsGuCAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ZixIDfP0FVNn"},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7c8accbc80f344e28403012bfdb60610","dca985180c2d4aca967f4059e18092ac","cb8949cee9244ad8a6e8855056873701","fad6f18e7d8141f28a655a99ebf9c4f8","9c4fc35e145348cd86b0c3ba69b41a44","25664f768c0444c0809d2dbc3c3c17d6","e2d2f4e0e1504ed4b4897ed488f7fb6d","0e8ea6ea8b2940c585b282326c1a5459","841cb4d3bc64494b84b65d967dacb07a","5b2cf497a91e439f94a1ee5bfabb146c","75f57d87b04d48109f0b05b72344cad0","37058808418e40b1b4064860f47ac2e4","fa053a87f3954ea3adc113671d8a3056","c2e45b330911469b8d5a1247443910cc","bca901c26edc46aa9fc4a9a3e64f4954","a1aac721be4e4971a500ff3571157b84","cd6d153297044bb297cb2486ae9287a1","58bfb06fa0c447b186184d5a90a25307","5e550ca73a8c420e808a787339f0feb3","3c34f2e19f9549f0b0f6a681cda16510","c832c9bfdd074fe997698aad74e354e6","dfe724827d7c40139f66f98276f49b59","7ad8655a428a4c449fab8c187d0b0038","f69bae6007ca4ef495727f5a0f28f106","94d50c5eb05a4e55b35537b21cf619a7","8ad49324eb0545e0a028ff90fd05bab9","94955c7220764f7cb39b2eca6d39aa10","ab1d117106384f7f9e2dfbe4cf80e55d","b09a7b04a9c444d9bac9bf96486362b8","f9a6026d63164faf9b69eb52516281c4","cd0d2302df66487ca435600a42786c96","13720550bafb4a4d96f1121aa6e2e30e","903e9d57b102487a8034291811c4785d","0b1edb2871ce44fc9803783c6ec6a502","91df3c63fe7e452987415a108542b392","b72522c30e20427f8c4b1f8b0466c50a","1f1a9c6efa294b72b22903a75cc105b7","be4ba6103eca470289f5bf02931e5e96","e1903a3500454dc39e923c9f971015df","2111fb98d1e647169c03915f7135d4d5","7b3a5966ce784bb4bcbeba778579100a","58b1cdd1bb044567b431749885bf4dae","1869a531648b46a6b846bf46dca3aac7","830ac025bab04d2083e39e946e21725e","f5a090387013433da6a968c2be1e6d9d","46014578a53b422ebe3ab6b7cae16a47","2b964e32ad264df48235a2def9401a96","983c1f4f4b204a75a3ecaef23414e615","05d8302a9b4e47e0afa2404f82f6637f","b70ff7addc8c49e8a87184cddc273d82","3e1e42a811ff4b8da0b95852dc7071fa","4fd0909b6a8848beb8de9d314dd2671c","edf5abe6149a49ddb3dbd9d27852a8f0","52ce9ea0c7ee4ed3a6e9726b1f57b835","a839048e592f472a96292c8f5c94f877"]},"executionInfo":{"elapsed":2098633,"status":"ok","timestamp":1640144512395,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"},"user_tz":-480},"id":"wUH-bnJN2ph-","outputId":"0c974454-50c3-4866-9bc8-01955c96a87c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c8accbc80f344e28403012bfdb60610","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37058808418e40b1b4064860f47ac2e4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ad8655a428a4c449fab8c187d0b0038","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b1edb2871ce44fc9803783c6ec6a502","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5a090387013433da6a968c2be1e6d9d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:43, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.207300</td>\n","      <td>0.137184</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.241900</td>\n","      <td>0.125384</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.123400</td>\n","      <td>0.105945</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.080900</td>\n","      <td>0.094190</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/price/checkpoint-500\n","Configuration saved in sameval2014/data/model/price/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/price/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/price/config.json\n","Model weights saved in sameval2014/data/model/price/pytorch_model.bin\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:43, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.763300</td>\n","      <td>0.579846</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.478500</td>\n","      <td>0.499453</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.351300</td>\n","      <td>0.485604</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.217900</td>\n","      <td>0.495365</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/anecdotes/checkpoint-500\n","Configuration saved in sameval2014/data/model/anecdotes/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/anecdotes/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/anecdotes/config.json\n","Model weights saved in sameval2014/data/model/anecdotes/pytorch_model.bin\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:43, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.499700</td>\n","      <td>0.509694</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.505300</td>\n","      <td>0.417509</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.271300</td>\n","      <td>0.439859</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.172100</td>\n","      <td>0.450723</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/food/checkpoint-500\n","Configuration saved in sameval2014/data/model/food/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/food/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/food/config.json\n","Model weights saved in sameval2014/data/model/food/pytorch_model.bin\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:43, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.345200</td>\n","      <td>0.280426</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.196400</td>\n","      <td>0.244750</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.121300</td>\n","      <td>0.218265</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.073900</td>\n","      <td>0.219018</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/ambience/checkpoint-500\n","Configuration saved in sameval2014/data/model/ambience/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/ambience/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/ambience/config.json\n","Model weights saved in sameval2014/data/model/ambience/pytorch_model.bin\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:45, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.325800</td>\n","      <td>0.232341</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.184300</td>\n","      <td>0.167020</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.121500</td>\n","      <td>0.190611</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.068000</td>\n","      <td>0.183390</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/service/checkpoint-500\n","Configuration saved in sameval2014/data/model/service/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/service/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/service/config.json\n","Model weights saved in sameval2014/data/model/service/pytorch_model.bin\n"]}],"source":["data_dir = 'sameval2014/data'\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","labels = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","\n","import torch\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import Trainer\n","from transformers import TrainingArguments\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","epochs = 4\n","batch_size = 24\n","learning_rate = 2e-5\n","\n","for aspect in aspects:\n","  training_args = TrainingArguments(\n","      output_dir = \"{}/model/{}\".format(data_dir, aspect),          \n","      num_train_epochs = epochs,              \n","      per_device_train_batch_size = batch_size,  \n","      per_device_eval_batch_size = batch_size,    \n","      weight_decay = 0.01,               \n","      logging_dir = \"{}/logs/{}\".format(data_dir, aspect),            \n","      logging_steps = 10,\n","      evaluation_strategy = 'epoch',\n","      learning_rate = learning_rate,\n","      seed=40\n","  )\n","\n","  # Get Data\n","  train_sentences, train_labels = get_train_data(data_dir, aspect)\n","  test_sentences, test_labels = get_test_data(data_dir, aspect)\n","\n","  # Tokenize sentences\n","  train_sentences_tokenized = tokenizer(train_sentences, truncation=True, padding=True)\n","  test_sentences_tokenized = tokenizer(test_sentences, truncation=True, padding=True)\n","\n","  # Create training data\n","  train_data = Semeval_Data(train_sentences_tokenized, train_labels)\n","  test_data = Semeval_Data(test_sentences_tokenized, test_labels)\n","\n","  # Load model\n","  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels))\n","\n","  trainer = Trainer(\n","      model,\n","      training_args,\n","      train_dataset=train_data,\n","      eval_dataset=test_data,\n","  )\n","\n","  trainer.train()\n","\n","  # Save model\n","  model.save_pretrained(\"{}/model/{}\".format(data_dir, aspect))"]},{"cell_type":"markdown","metadata":{"id":"30ZnyX4PKDUl"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"aAp8RI9EKDYe"},"source":["## Make Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":59211,"status":"ok","timestamp":1640145051617,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"},"user_tz":-480},"id":"MqYsgy3keR9q","outputId":"7ed46eab-c997-4483-b46d-649ca77e76da"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file sameval2014/data/model/price/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/price\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/price/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/price.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:08]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file sameval2014/data/model/anecdotes/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/anecdotes\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/anecdotes/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/anecdotes.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:07]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file sameval2014/data/model/food/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/food\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/food/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/food.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:07]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file sameval2014/data/model/ambience/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/ambience\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/ambience/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/ambience.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:07]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file sameval2014/data/model/service/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/service\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/service/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/service.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:07]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from scipy.special import softmax\n","from transformers import AutoModelForSequenceClassification\n","from transformers import Trainer\n","from transformers import AutoTokenizer\n","\n","checkpoint = \"bert-base-uncased\"\n","data_dir = 'sameval2014/data'\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","labels = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","\n","for aspect in aspects:\n","  test_sentences, test_labels = get_test_data(data_dir, aspect)\n","\n","  # Tokenize sentences\n","  test_encodings = tokenizer(test_sentences, truncation=True, padding=True)\n","\n","  # Create training data\n","  test_dataset = Semeval_Data(test_encodings, test_labels)\n","\n","  # Load model\n","  model = AutoModelForSequenceClassification.from_pretrained(\"{}/model/{}\".format(data_dir, aspect))\n","  trainer = Trainer(model=model)\n","  trainer.model = model.cuda()\n","  predictions = trainer.predict(test_dataset)\n","\n","  probabilities = [softmax(prediction) for prediction in predictions.predictions]\n","  y_pred = [np.argmax(x) for x in probabilities]\n","\n","  df = pd.DataFrame()\n","  df['predicted_label'] = y_pred\n","  df = pd.concat([df,pd.DataFrame(probabilities, index=df.index, columns=labels)], axis=1)\n","  df.to_csv(\"{}/predictions/{}.csv\".format(data_dir, aspect), index=False)"]},{"cell_type":"markdown","metadata":{"id":"KyOLiUH4EMIh"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"SpwVIA655i3X"},"source":["## Results Evaluation"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","labels = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","data_dir = 'sameval2014/data'"],"metadata":{"id":"2dtBl7EptE76","executionInfo":{"status":"ok","timestamp":1647752663279,"user_tz":-480,"elapsed":764,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dKZ02pXX6SMv"},"source":["### Utility Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"wcJdx9ziO9dZ","executionInfo":{"status":"ok","timestamp":1647752664005,"user_tz":-480,"elapsed":1,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}}},"outputs":[],"source":["def get_predictions(data_dir):\n","  print(\"Getting predicted labels from {}\".format(data_dir))\n","  predicted_labels = []\n","  scores = []\n","\n","  for aspect in aspects:\n","      df = pd.read_csv(\"{}/predictions/{}.csv\".format(data_dir, aspect), header=0)\n","      pred_labels = df['predicted_label'].values.tolist()\n","      score = df[labels].values.tolist()\n","      predicted_labels.extend(pred_labels)\n","      scores.extend(score)\n","  return predicted_labels, scores\n","\n","def get_true_labels(data_dir):\n","  print(\"Getting actual labels from {}\\n\".format(data_dir))\n","  test_labels = []\n","  for aspect in aspects:\n","      df = pd.read_csv(\"{}/{}/test.csv\".format(data_dir, aspect), header=0, sep=\"\\t\")\n","      test_label = df['label_id'].values.tolist()\n","      test_labels.extend(test_label)\n","\n","  return test_labels"]},{"cell_type":"markdown","metadata":{"id":"oDnSN31Y6U0e"},"source":["### Aspect Detection"]},{"cell_type":"code","source":["def aspect_detection_prf(test_labels, predicted_labels):\n","    true_postive = 0   # aspect present, predicted present\n","    false_postive = 0  # aspect not present, predicted present\n","    false_negative = 0  # aspect present, predicted not present\n","    true_negative = 0   # aspect not present, predicted not present\n","\n","    for i in range(len(predicted_labels)):\n","      if predicted_labels[i] == 4 and test_labels[i] == 4: # true negative\n","        true_negative += 1\n","      elif predicted_labels[i] == 4 and test_labels[i] != 4: # false negative\n","        false_negative += 1\n","      elif predicted_labels[i] != 4 and test_labels[i] == 4: # false postive\n","        false_postive += 1\n","      elif predicted_labels[i] != 4 and test_labels[i] != 4: # true postive\n","        true_postive += 1\n","\n","    precision = true_postive / (true_postive + false_postive)\n","    recall = true_postive / (true_postive + false_negative)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return  precision, recall, f1"],"metadata":{"id":"XC17LPN_5D_-","executionInfo":{"status":"ok","timestamp":1647752664268,"user_tz":-480,"elapsed":1,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Aspect Detection Evaluation\n","predicted_labels, scores = get_predictions(data_dir)\n","test_labels = get_true_labels(data_dir)\n","\n","precision, recall, accuracy = aspect_detection_prf(test_labels,predicted_labels)\n","print(\"Semeval aspect precision: {}\".format(precision))\n","print(\"Semeval aspect recall: {}\".format(recall))\n","print(\"Semeval aspect F1: {}\".format(accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rto5OuKM7JRa","executionInfo":{"status":"ok","timestamp":1647752677612,"user_tz":-480,"elapsed":731,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"eed123b5-f77f-40d8-8ade-fd3f23463b30"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Getting predicted labels from sameval2014/data\n","Getting actual labels from sameval2014/data\n","\n","Semeval aspect precision: 0.9303991811668373\n","Semeval aspect recall: 0.8868292682926829\n","Semeval aspect F1: 0.908091908091908\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5yOIKDRZzVwn"},"outputs":[],"source":["# def aspect_detection_prf(test_labels, predicted_labels):\n","#     num_total_intersection = 0\n","#     num_total_test_aspects = 0\n","#     num_total_predicted_aspects = 0\n","#     num_examples = len(test_labels) // 5\n","#     for i in range(num_examples):\n","#         test_aspects = set()\n","#         predicted_aspects = set()\n","#         for j in range(5):\n","#             if test_labels[i * 5 + j] != 4:\n","#                 test_aspects.add(j)\n","#             if predicted_labels[i * 5 + j] != 4:\n","#                 predicted_aspects.add(j)\n","#         if len(test_aspects) == 0:\n","#             continue\n","#         intersection = test_aspects.intersection(predicted_aspects)\n","#         num_total_test_aspects += len(test_aspects)\n","#         num_total_predicted_aspects += len(predicted_aspects)\n","#         num_total_intersection += len(intersection)\n","#     precision = num_total_intersection / num_total_predicted_aspects\n","#     recall = num_total_intersection / num_total_test_aspects\n","#     micro_F1 = (2 * precision * recall) / (precision + recall)\n","#     return  precision, recall, micro_F1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rD1pdm8Y8nC"},"outputs":[],"source":["# # Aspect Detection Evaluation\n","# predicted_labels, scores = get_predictions(data_dir)\n","# test_labels = get_true_labels(data_dir)\n","\n","# precision, recall, micro_F1 = aspect_detection_prf(test_labels,predicted_labels)\n","# print(\"Semeval aspect precision: {}\".format(precision))\n","# print(\"Semeval aspect recall: {}\".format(recall))\n","# print(\"Semeval aspect micro F1: {}\".format(micro_F1))"]},{"cell_type":"markdown","metadata":{"id":"gJRXfQdY6XZX"},"source":["### Sentiment Polarity"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Xl7tFKYZ5m-d","executionInfo":{"status":"ok","timestamp":1647752709942,"user_tz":-480,"elapsed":489,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}}},"outputs":[],"source":["def get_n_way_accuracy(test_labels, predicted_labels, scores, labels_range):\n","  no_examples = 0\n","  no_true_examples = 0\n","  for i in range(len(test_labels)):\n","    if test_labels[i] in labels_range: # inside the labels range\n","      temp_label = predicted_labels[i] # Extract the label\n","\n","      # If predicted label is not in labels range, compute new predicted label after making the score 0 for labels not in labels range\n","      if temp_label not in labels_range:\n","          new_scores = [0 if index not in labels_range else scores[i][index] for index in range(len(scores[i]))]\n","          temp_label = np.argmax(new_scores)\n","            \n","\n","      if test_labels[i] == temp_label:\n","          no_true_examples += 1\n","      no_examples += 1\n","    \n","  accuracy = no_true_examples / no_examples\n","  return accuracy"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1647752709942,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"},"user_tz":-480},"id":"DMIen7ve6LBx","outputId":"13678f85-c1cb-4309-c896-58e21faf628f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Getting predicted labels from sameval2014/data\n","Getting actual labels from sameval2014/data\n","\n"]}],"source":["predicted_labels, scores = get_predictions(data_dir)\n","test_labels = get_true_labels(data_dir)\n","binary_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,2))\n","three_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2))\n","four_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2,3))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1647752710787,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"},"user_tz":-480},"id":"50oTvqq07k-y","outputId":"d66579b3-0fa5-4c72-80a5-4314851f326b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Binary Accuracy: 0.9271899886234357\n","Three Way Accuracy: 0.8612538540596094\n","Four Way Accuracy: 0.8214634146341463\n"]}],"source":["print(\"Binary Accuracy: {}\".format(binary_accuracy))\n","print(\"Three Way Accuracy: {}\".format(three_accuracy))\n","print(\"Four Way Accuracy: {}\".format(four_accuracy))"]},{"cell_type":"markdown","source":["### Aspect level evaluation"],"metadata":{"id":"7-SWS0oU8npM"}},{"cell_type":"code","source":["def get_predictions_aspect(data_dir, aspect):\n","  #print(\"Getting predicted labels from {}/{}\".format(data_dir, aspect))\n","  predicted_labels = []\n","  scores = []\n","\n","  df = pd.read_csv(\"{}/predictions/{}.csv\".format(data_dir, aspect), header=0)\n","  predicted_labels = df['predicted_label'].values.tolist()\n","  scores = df[labels].values.tolist()\n","  return predicted_labels, scores\n","\n","def get_true_labels_aspect(data_dir, aspect):\n","  #print(\"Getting actual labels from {}/{}\\n\".format(data_dir, aspect))\n","\n","  df = pd.read_csv(\"{}/{}/test.csv\".format(data_dir, aspect), header=0, sep=\"\\t\")\n","  test_labels = df['label_id'].values.tolist()\n","\n","  return test_labels\n","\n","def aspect_detection_evaluation_aspect(data_dir, aspect):\n","    predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","    test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","    precision, recall, micro_F1 = aspect_detection_prf(test_labels,predicted_labels)\n","    print(\"Semeval aspect precision: {}\".format(precision))\n","    print(\"Semeval aspect recall: {}\".format(recall))\n","    print(\"Semeval aspect micro F1: {}\".format(micro_F1))\n","    return"],"metadata":{"id":"4eIn3VIsRHHm","executionInfo":{"status":"ok","timestamp":1647752713003,"user_tz":-480,"elapsed":1,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Aspect Detection Evaluation per aspect\n","data_dir = \"sameval2014/data\"\n","evaluation_aspect_detection = {'Precision': [], 'Recall': [], 'F1': []}\n","evaluation_sentiment_polarity = {'Binary Accuracy': [], 'Three Way Accuracy': [], 'Four Way Accuracy': []}\n","for aspect in aspects:\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  # Aspect detection\n","  precision, recall, f1 = aspect_detection_prf(test_labels,predicted_labels)\n","  evaluation_aspect_detection['Precision'].append(precision)\n","  evaluation_aspect_detection['Recall'].append(recall)\n","  evaluation_aspect_detection['F1'].append(f1)\n","\n","  # Sentiment Polarity\n","  binary_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,2))\n","  three_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2))\n","  four_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2,3))\n","  evaluation_sentiment_polarity['Binary Accuracy'].append(binary_accuracy)\n","  evaluation_sentiment_polarity['Three Way Accuracy'].append(three_accuracy)\n","  evaluation_sentiment_polarity['Four Way Accuracy'].append(four_accuracy)\n","\n","df_aspect_detection = pd.DataFrame(data=evaluation_aspect_detection, index=aspects)\n","df_sentiment_polarity = pd.DataFrame(data=evaluation_sentiment_polarity, index=aspects)\n","\n","print(\"Aspect Detection\")\n","print(df_aspect_detection) \n","\n","print(\"\\nSentiment Polarity\")\n","print(df_sentiment_polarity)  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5SvpkO5D-qTM","executionInfo":{"status":"ok","timestamp":1647752713933,"user_tz":-480,"elapsed":448,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"ee317d2d-8be6-4981-b454-e014e3127946"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Aspect Detection\n","           Precision    Recall        F1\n","price       0.987179  0.927711  0.956522\n","anecdotes   0.894472  0.760684  0.822171\n","food        0.955990  0.935407  0.945586\n","ambience    0.896552  0.881356  0.888889\n","service     0.908571  0.924419  0.916427\n","\n","Sentiment Polarity\n","           Binary Accuracy  Three Way Accuracy  Four Way Accuracy\n","price             0.848101            0.837500           0.807229\n","anecdotes         0.922619            0.799087           0.747863\n","food              0.938005            0.873134           0.846890\n","ambience          0.938144            0.866667           0.779661\n","service           0.939024            0.922156           0.895349\n"]}]},{"cell_type":"code","source":["# Aspect Detection Evaluation per aspect\n","data_dir = \"sameval2014/data\"\n","evaluation_aspect_detection = {'Precision': [], 'Recall': [], 'Micro F1': []}\n","evaluation_sentiment_polarity = {'Binary Accuracy': [], 'Three Way Accuracy': [], 'Four Way Accuracy': []}\n","for aspect in aspects:\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  # Aspect detection\n","  precision, recall, micro_F1 = aspect_detection_prf(test_labels,predicted_labels)\n","  evaluation_aspect_detection['Precision'].append(precision)\n","  evaluation_aspect_detection['Recall'].append(recall)\n","  evaluation_aspect_detection['Micro F1'].append(micro_F1)\n","\n","  # Sentiment Polarity\n","  binary_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,2))\n","  three_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2))\n","  four_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2,3))\n","  evaluation_sentiment_polarity['Binary Accuracy'].append(binary_accuracy)\n","  evaluation_sentiment_polarity['Three Way Accuracy'].append(three_accuracy)\n","  evaluation_sentiment_polarity['Four Way Accuracy'].append(four_accuracy)\n","\n","df_aspect_detection = pd.DataFrame(data=evaluation_aspect_detection, index=aspects)\n","df_sentiment_polarity = pd.DataFrame(data=evaluation_sentiment_polarity, index=aspects)\n","\n","print(\"Aspect Detection\")\n","print(df_aspect_detection) \n","\n","print(\"\\nSentiment Polarity\")\n","print(df_sentiment_polarity)  "],"metadata":{"id":"iIVXrBM37hO4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647752716657,"user_tz":-480,"elapsed":318,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"1226d069-9e42-45af-d879-f819199cc1b7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Aspect Detection\n","           Precision    Recall  Micro F1\n","price       0.987179  0.927711  0.956522\n","anecdotes   0.894472  0.760684  0.822171\n","food        0.955990  0.935407  0.945586\n","ambience    0.896552  0.881356  0.888889\n","service     0.908571  0.924419  0.916427\n","\n","Sentiment Polarity\n","           Binary Accuracy  Three Way Accuracy  Four Way Accuracy\n","price             0.848101            0.837500           0.807229\n","anecdotes         0.922619            0.799087           0.747863\n","food              0.938005            0.873134           0.846890\n","ambience          0.938144            0.866667           0.779661\n","service           0.939024            0.922156           0.895349\n"]}]},{"cell_type":"markdown","source":["### Further Exploration"],"metadata":{"id":"qC_NQEfpOVOX"}},{"cell_type":"code","source":["sentences = pd.read_csv(\"{}/price/test.csv\".format(data_dir),header=0, sep=\"\\t\")['text'].tolist()\n","len(sentences)"],"metadata":{"id":"kUfc7LCghpdy","executionInfo":{"status":"ok","timestamp":1647752717183,"user_tz":-480,"elapsed":2,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3cb743cc-4e24-4bdf-fdaf-908dee94aae0"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["800"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["#### Aspect labels detected when not present (false postive)"],"metadata":{"id":"9n1MhriL0cj-"}},{"cell_type":"code","source":["incorrect_aspect_detection = {\"Positive\":[], \"Negative\":[], \"Neutral\":[]}\n","\n","for aspect in aspects:\n","  print(\"Evaluation for {}\".format(aspect))\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  pos = 0\n","  neg = 0\n","  neutral = 0\n","  for i in range(len(predicted_labels)):\n","    predicted_label = predicted_labels[i]\n","    if predicted_label != 4 and test_labels[i] == 4:\n","      if predicted_label == 0:\n","        pos = pos + 1\n","      elif predicted_label == 1:\n","        neutral = neutral + 1\n","      elif predicted_label == 2:\n","        neg = neg + 1\n","\n","  incorrect_aspect_detection['Positive'].append(pos)\n","  incorrect_aspect_detection['Negative'].append(neg)\n","  incorrect_aspect_detection['Neutral'].append(neutral)\n","\n","print(pd.DataFrame(data=incorrect_aspect_detection, index=aspects))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_HWEacznKhX","executionInfo":{"status":"ok","timestamp":1647752721319,"user_tz":-480,"elapsed":256,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"85e3f733-951c-4573-c6ff-31b434d78bf2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation for price\n","Evaluation for anecdotes\n","Evaluation for food\n","Evaluation for ambience\n","Evaluation for service\n","           Positive  Negative  Neutral\n","price             1         0        0\n","anecdotes        12         3        6\n","food             14         4        0\n","ambience         10         2        0\n","service           8         8        0\n"]}]},{"cell_type":"markdown","source":["#### Sentences of aspect detected when not present"],"metadata":{"id":"sYDibA0tyKy9"}},{"cell_type":"code","source":["incorrect_aspect_detection_sentences = {\"price\":[], \"anecdotes\":[], \"food\":[], \"ambience\":[], \"service\":[]}\n","for aspect in aspects:\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  for i in range(len(predicted_labels)):\n","    predicted_label = predicted_labels[i]\n","    if predicted_label != 4 and test_labels[i] == 4:\n","      incorrect_aspect_detection_sentences[aspect].append(sentences[i])\n","\n","incorrect_aspect_detection_sentences['anecdotes']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bt6D2WL_oXdj","executionInfo":{"status":"ok","timestamp":1647759080218,"user_tz":-480,"elapsed":289,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"f936c819-0a50-4319-86dd-bcb8a13d7fcd"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"Stay away if you're claustrophobic\",\n"," 'i dont know what some people who rave about this hot dog are talking about.',\n"," \"I can't stand this dungeon.\",\n"," \"You're going to drop some coin, but completely worth it, especially if you are looking to impress someone.\",\n"," 'My order is on my table even on a busy friday night within 10 minutes (at the most) of hanging up the phone.',\n"," 'We came across this restaurant by accident while at a DUMBO art festival and thoroughly enjoyed our meal.',\n"," 'If you are a Tequila fan you will not be disappointed.',\n"," 'How can hope to stay in business with service like this?',\n"," 'My daughter and I left feeling satisfied (not stuffed) and it felt good to know we had a healthy lunch.',\n"," 'It first came well done, and I politely sent it back.',\n"," \"If you're lucky and there's not a private party going on back there, you'll get a chance to really chill out.\",\n"," 'Had a great meal there this weekend before heading to the movies!',\n"," 'We went there, no one greeted us.',\n"," 'We spent so much time getting elbowed around while we waited we left.',\n"," \"For someone who used to hate Indian food, Baluchi's has changed my mid.\",\n"," 'I went for restaurant week and ordered off the prix fixe menu',\n"," 'Despite the 5th ave address there are no frills whick I also love.',\n"," \"If you're in the neighborhood, definitely stop by for a great meal.\",\n"," 'However, there is just something so great about being outdoors, in great landscaping, enjoying a casual drink that makes going to this place worthwhile.',\n"," \"But now that it's under new ownership and has been renovated, it looks great.\",\n"," 'I stopped by for some brunch today and had the vegan cranberry pancakes and some rice milk.']"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["#### Aspect labels not detected when present (false negative)"],"metadata":{"id":"YdoY10j22HJB"}},{"cell_type":"code","source":["incorrect_aspect_detection = {\"Count\":[]}\n","\n","for aspect in aspects:\n","  print(\"Evaluation for {}\".format(aspect))\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  count = 0\n","  for i in range(len(predicted_labels)):\n","    predicted_label = predicted_labels[i]\n","    if predicted_label == 4 and test_labels[i] != 4:\n","      count += 1\n","  incorrect_aspect_detection['Count'].append(count)\n","\n","print(pd.DataFrame(data=incorrect_aspect_detection, index=aspects))\n"],"metadata":{"id":"abLmMl5GnKqc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647755158306,"user_tz":-480,"elapsed":263,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"1bd3e09b-6846-4c82-cb6a-87b4aac833b3"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation for price\n","Evaluation for anecdotes\n","Evaluation for food\n","Evaluation for ambience\n","Evaluation for service\n","           Count\n","price          6\n","anecdotes     56\n","food          27\n","ambience      14\n","service       13\n"]}]},{"cell_type":"markdown","source":["#### Sentences of aspect not detected when present"],"metadata":{"id":"oageDDz49Txt"}},{"cell_type":"code","source":["incorrect_aspect_detection_sentences = {\"price\":[], \"anecdotes\":[], \"food\":[], \"ambience\":[], \"service\":[]}\n","for aspect in aspects:\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  for i in range(len(predicted_labels)):\n","    predicted_label = predicted_labels[i]\n","    if predicted_label == 4 and test_labels[i] != 4:\n","      incorrect_aspect_detection_sentences[aspect].append(sentences[i])\n","\n","incorrect_aspect_detection_sentences['anecdotes']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lb952In4-Pxv","executionInfo":{"status":"ok","timestamp":1647758992055,"user_tz":-480,"elapsed":289,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"f72997f3-3154-46fe-9743-b220df4aa3e9"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I trust the people at Go Sushi, it never disappoints.',\n"," 'From the beginning, we were met by friendly staff memebers, and the convienent parking at Chelsea Piers made it easy for us to get to the boat.',\n"," 'We enjoyed ourselves thoroughly and will be going back for the desserts ....',\n"," 'As we waited I watched 3 seperate groups of diners discuss how dissapointed they also were.',\n"," \"Curioni's Pizza has been around since the 1920's.\",\n"," 'Over priced, overrated....stay away!',\n"," \"Since I cook for a living, I'm very fussy about the food I eat in restaurants.\",\n"," \"The place just isn't ready for prime time.\",\n"," 'And you never get the feeling that you need to leave.',\n"," \"It's cute and simple and I loved it.\",\n"," 'THE SERVICE IS PERFECT TOO  NOTHING WRONG IN THIS ITALIAN/FRENCH RESTAURANT',\n"," 'The service was a bit slow and the portions are a bit small so if you are hungry and in a rush, this is not the place for you.',\n"," 'The ambiance was pretty cool, but not worth the hassle.',\n"," \"El Nidos one of the best restaurants in New York which I've ever been to, has a great variety of tasty, mouth watering pizza's.\",\n"," 'The service was pretty poor all around, the food was well below average relative to the cost, and outside there is a crazy bum who harasses every customer who leaves the place.',\n"," 'You must try Odessa stew or Rabbit stew; salads-all good; and kompot is soo refreshing during the hot summer day (they make it the way my mom does, reminds me of home a lot).',\n"," 'The manager then told us we could order from whatever menu we wanted but by that time we were so annoyed with the waiter and the resturant that we let and went some place else.',\n"," \"it's good but, like the movie is never as good as the book, this is same analogy.\",\n"," \"expectations were real hi reading the '10's'.\",\n"," 'Cool atmosphere but such a let down.',\n"," 'We ended up having to just leave because we were essentially being ignored by the wait staff--even though the rest of the restaurant was largely empty.',\n"," 'Everything about this place is adorable - even the bathroom!',\n"," 'THEY HAVE WAITERS ON THE SIDEWALK TRYING TO PULL YOU IN WHICH MADE US SUSPICIOUS.',\n"," 'IT WAS OUR ONLY OPPORTUNITY TO VISIT AND WANTED AN AUTHENTIC ITALIAN MEAL.',\n"," \"It took 100 years for Parisi to get around to making pizza (at least I don't think they ever made it before this year)...but it was worth the wait.\",\n"," 'Ask for the round corner table next to the large window.',\n"," 'The lack of AC and the fact that there are a million swarming bodies (although everyone is polite and no one is pushing) is a slight turn off.',\n"," \"Unfortunately, unless you live in the neighborhood, it's not in a convenient location but is more like a hidden treasure.\",\n"," \"Both were very good, but nothing you couldn't get at similar restaurant.\",\n"," 'Service was awful - mostly because staff were overwhelmed on a Saturday night.',\n"," 'perfect for a quick meal.',\n"," \"My only complaint might be the fortune cookies - I've never had a cookie predict bad luck for me before I visited Kar.\",\n"," 'Good for a quick sushi lunch.',\n"," 'I would definitely go back -- if only for some of those exotic martinis on the blackboard.',\n"," 'then she made a fuss about not being able to add 1 or 2 chairs on either end of the table for additional people.',\n"," 'Give it a try, menu is typical French but varied.',\n"," 'Actually, everyone who worked there seemed to be annoyed.',\n"," 'Best Chinese on the Upper East, prompt delivery, good value.',\n"," 'Great restaurant, and even greater food!',\n"," 'It took a bigger bite from my wallet than my appetite - I would not reccomend this to anyone that I would want to talk to me again!!',\n"," \"It's always quiet because it's awful.\",\n"," 'The restaurant could use a little paint, but all in all a great sushi place!',\n"," 'The decoration was feeling like we was on the Cairo, actually the street is part of that adventure.',\n"," 'Bring your date and a bottle of wine!',\n"," 'The food is consistant and good but how it got name Best Diner In Manhattan is beyond me.',\n"," 'Had dinner here on a Friday and the food was great.',\n"," 'I was on jury duty, rode my bike up Centre Street on my lunch break and came across this great little place with awesome chicken tacos and Hibiscus lemonade.',\n"," 'good place to hang out during the day after shopping or to grab a simple soup or classic french dish over a glass of wine.',\n"," 'We were seated promptly in close proximity to the dance floor.',\n"," 'If you are here as a pre-show meal, hop in a cab and take the extra 10 minutes to go to the uptown location.',\n"," \"Slow service, but when you're hanging around with groups of 10 or 20, who really notices?\",\n"," 'Two complaints- their appetizer selection stinks, it would be nice to get some mozzarella sticks on the menu.',\n"," \"I don't remember what we had; nothing was memorable.\",\n"," \"The new menu has a few creative items,they were smart enough to keep some of the old favorites (but they raised the prices), the staff is friendly most of the time, but I must agree with the person that wrote about their favorite words: No, can't, sorry..., boy, they won't bend the rules for anyone.\",\n"," \"It' only open for lunch but the food is so good!\",\n"," 'I asked repeatedly what the status of the meal was and was pretty much grunted at by the unbelievably rude waiter.']"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## Prediction Pipeline"],"metadata":{"id":"JkKAavTpE8qU"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import Trainer\n","from scipy.special import softmax\n","\n","\n","data_dir = 'sameval2014/data'\n","semeval_sentiments = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","semeval_label2id = {\"positive\": 0, \"neutral\": 1, \"negative\": 2, \"conflict\": 3, \"none\": 4}\n","checkpoint = \"bert-base-uncased\"\n","\n","# Load models\n","models = []\n","for aspect in aspects: \n","  models.append(AutoModelForSequenceClassification.from_pretrained(\"{}/model/{}\".format(data_dir, aspect)))\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nOnlJngSFQ88","executionInfo":{"status":"ok","timestamp":1645958791303,"user_tz":-480,"elapsed":20468,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"70be57dd-9607-437b-d59d-73fed875f4dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file sameval2014/data/model/price/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/price\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/price/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/price.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file sameval2014/data/model/anecdotes/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/anecdotes\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/anecdotes/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/anecdotes.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file sameval2014/data/model/food/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/food\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/food/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/food.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file sameval2014/data/model/ambience/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/ambience\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/ambience/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/ambience.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file sameval2014/data/model/service/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/service\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/service/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/service.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"code","source":["text = \"The food was amazing but it was abit expensive\"\n","\n","# tokenize sentence\n","test_encodings = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n","\n","def predict(models, test_encodings, aspects):\n","    sentiments = []\n","\n","    # Make predictions\n","    for i in range(len(models)):\n","      outputs = models[i](**test_encodings)\n","      probabilities = softmax(outputs.logits[0].detach())\n","      label = int(np.argmax(probabilities))\n","      sentiment = semeval_sentiments[label]\n","      sentiments.append(sentiment)\n","\n","    return sentiments\n","  \n","sentiments = predict(models, test_encodings, aspects)\n","\n","print(\"Aspects\")              \n","for i in range(len(sentiments)):\n","  if sentiments[i] != 'none':\n","    aspect = aspects[i]\n","    print('({}, {})'.format(aspect, sentiments[i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5KUs5M_KnNSp","executionInfo":{"status":"ok","timestamp":1645958943987,"user_tz":-480,"elapsed":1913,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"41f55cd8-315b-4378-8833-ebe25ad16c1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Aspects\n","(price, negative)\n","(food, positive)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"23l18GNswL5w"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Aux Sentences (single)","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO8os71askA2oRUzw6HKmI8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"05d8302a9b4e47e0afa2404f82f6637f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b1edb2871ce44fc9803783c6ec6a502":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91df3c63fe7e452987415a108542b392","IPY_MODEL_b72522c30e20427f8c4b1f8b0466c50a","IPY_MODEL_1f1a9c6efa294b72b22903a75cc105b7"],"layout":"IPY_MODEL_be4ba6103eca470289f5bf02931e5e96"}},"0e8ea6ea8b2940c585b282326c1a5459":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13720550bafb4a4d96f1121aa6e2e30e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1869a531648b46a6b846bf46dca3aac7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f1a9c6efa294b72b22903a75cc105b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1869a531648b46a6b846bf46dca3aac7","placeholder":"​","style":"IPY_MODEL_830ac025bab04d2083e39e946e21725e","value":" 455k/455k [00:00&lt;00:00, 1.19MB/s]"}},"2111fb98d1e647169c03915f7135d4d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25664f768c0444c0809d2dbc3c3c17d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b964e32ad264df48235a2def9401a96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fd0909b6a8848beb8de9d314dd2671c","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_edf5abe6149a49ddb3dbd9d27852a8f0","value":440473133}},"37058808418e40b1b4064860f47ac2e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa053a87f3954ea3adc113671d8a3056","IPY_MODEL_c2e45b330911469b8d5a1247443910cc","IPY_MODEL_bca901c26edc46aa9fc4a9a3e64f4954"],"layout":"IPY_MODEL_a1aac721be4e4971a500ff3571157b84"}},"3c34f2e19f9549f0b0f6a681cda16510":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e1e42a811ff4b8da0b95852dc7071fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46014578a53b422ebe3ab6b7cae16a47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b70ff7addc8c49e8a87184cddc273d82","placeholder":"​","style":"IPY_MODEL_3e1e42a811ff4b8da0b95852dc7071fa","value":"Downloading: 100%"}},"4fd0909b6a8848beb8de9d314dd2671c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52ce9ea0c7ee4ed3a6e9726b1f57b835":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58b1cdd1bb044567b431749885bf4dae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58bfb06fa0c447b186184d5a90a25307":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b2cf497a91e439f94a1ee5bfabb146c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e550ca73a8c420e808a787339f0feb3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75f57d87b04d48109f0b05b72344cad0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ad8655a428a4c449fab8c187d0b0038":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f69bae6007ca4ef495727f5a0f28f106","IPY_MODEL_94d50c5eb05a4e55b35537b21cf619a7","IPY_MODEL_8ad49324eb0545e0a028ff90fd05bab9"],"layout":"IPY_MODEL_94955c7220764f7cb39b2eca6d39aa10"}},"7b3a5966ce784bb4bcbeba778579100a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c8accbc80f344e28403012bfdb60610":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dca985180c2d4aca967f4059e18092ac","IPY_MODEL_cb8949cee9244ad8a6e8855056873701","IPY_MODEL_fad6f18e7d8141f28a655a99ebf9c4f8"],"layout":"IPY_MODEL_9c4fc35e145348cd86b0c3ba69b41a44"}},"830ac025bab04d2083e39e946e21725e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"841cb4d3bc64494b84b65d967dacb07a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ad49324eb0545e0a028ff90fd05bab9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13720550bafb4a4d96f1121aa6e2e30e","placeholder":"​","style":"IPY_MODEL_903e9d57b102487a8034291811c4785d","value":" 226k/226k [00:00&lt;00:00, 5.47kB/s]"}},"903e9d57b102487a8034291811c4785d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91df3c63fe7e452987415a108542b392":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1903a3500454dc39e923c9f971015df","placeholder":"​","style":"IPY_MODEL_2111fb98d1e647169c03915f7135d4d5","value":"Downloading: 100%"}},"94955c7220764f7cb39b2eca6d39aa10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94d50c5eb05a4e55b35537b21cf619a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9a6026d63164faf9b69eb52516281c4","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd0d2302df66487ca435600a42786c96","value":231508}},"983c1f4f4b204a75a3ecaef23414e615":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52ce9ea0c7ee4ed3a6e9726b1f57b835","placeholder":"​","style":"IPY_MODEL_a839048e592f472a96292c8f5c94f877","value":" 420M/420M [00:21&lt;00:00, 19.9MB/s]"}},"9c4fc35e145348cd86b0c3ba69b41a44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1aac721be4e4971a500ff3571157b84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a839048e592f472a96292c8f5c94f877":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab1d117106384f7f9e2dfbe4cf80e55d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b09a7b04a9c444d9bac9bf96486362b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b70ff7addc8c49e8a87184cddc273d82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b72522c30e20427f8c4b1f8b0466c50a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b3a5966ce784bb4bcbeba778579100a","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58b1cdd1bb044567b431749885bf4dae","value":466062}},"bca901c26edc46aa9fc4a9a3e64f4954":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c832c9bfdd074fe997698aad74e354e6","placeholder":"​","style":"IPY_MODEL_dfe724827d7c40139f66f98276f49b59","value":" 570/570 [00:00&lt;00:00, 2.86kB/s]"}},"be4ba6103eca470289f5bf02931e5e96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2e45b330911469b8d5a1247443910cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e550ca73a8c420e808a787339f0feb3","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c34f2e19f9549f0b0f6a681cda16510","value":570}},"c832c9bfdd074fe997698aad74e354e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb8949cee9244ad8a6e8855056873701":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e8ea6ea8b2940c585b282326c1a5459","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_841cb4d3bc64494b84b65d967dacb07a","value":28}},"cd0d2302df66487ca435600a42786c96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd6d153297044bb297cb2486ae9287a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dca985180c2d4aca967f4059e18092ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25664f768c0444c0809d2dbc3c3c17d6","placeholder":"​","style":"IPY_MODEL_e2d2f4e0e1504ed4b4897ed488f7fb6d","value":"Downloading: 100%"}},"dfe724827d7c40139f66f98276f49b59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1903a3500454dc39e923c9f971015df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2d2f4e0e1504ed4b4897ed488f7fb6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edf5abe6149a49ddb3dbd9d27852a8f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5a090387013433da6a968c2be1e6d9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46014578a53b422ebe3ab6b7cae16a47","IPY_MODEL_2b964e32ad264df48235a2def9401a96","IPY_MODEL_983c1f4f4b204a75a3ecaef23414e615"],"layout":"IPY_MODEL_05d8302a9b4e47e0afa2404f82f6637f"}},"f69bae6007ca4ef495727f5a0f28f106":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab1d117106384f7f9e2dfbe4cf80e55d","placeholder":"​","style":"IPY_MODEL_b09a7b04a9c444d9bac9bf96486362b8","value":"Downloading: 100%"}},"f9a6026d63164faf9b69eb52516281c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa053a87f3954ea3adc113671d8a3056":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd6d153297044bb297cb2486ae9287a1","placeholder":"​","style":"IPY_MODEL_58bfb06fa0c447b186184d5a90a25307","value":"Downloading: 100%"}},"fad6f18e7d8141f28a655a99ebf9c4f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b2cf497a91e439f94a1ee5bfabb146c","placeholder":"​","style":"IPY_MODEL_75f57d87b04d48109f0b05b72344cad0","value":" 28.0/28.0 [00:00&lt;00:00, 213B/s]"}}}}},"nbformat":4,"nbformat_minor":0}