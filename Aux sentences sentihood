{"cells":[{"cell_type":"markdown","metadata":{"id":"ImdaUDI3782w"},"source":["## Mount Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWP748mQ8A17","executionInfo":{"status":"ok","timestamp":1647186824838,"user_tz":-480,"elapsed":3511,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"d84b3c64-fd17-4619-8dbe-fdb0dafdf44c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# %cd /content/drive/My Drive/Ntu/nndl/CZ4042 Final Project\n","\n","%cd /content/drive/My Drive/"]},{"cell_type":"markdown","metadata":{"id":"GHPIs_bEfR2a"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USqqTyfxA5H-","outputId":"7dd7b279-5b67-4681-b844-49d2e7fcc026","executionInfo":{"status":"ok","timestamp":1647186830704,"user_tz":-480,"elapsed":5868,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"zTgBQp_TERDx"},"source":["## Save Data files"]},{"cell_type":"code","source":["import os\n","import json\n","import pandas as pd\n","import re\n","import xml.etree.ElementTree as ET\n","from pathlib import Path\n","\n","\n","###############################\n","### Sentihood #################\n","###############################\n","\n","sentihood_sentiments = [\"none\", \"positive\", \"negative\"]\n","sentihood_locations = [\"location - 1\", \"location - 2\"]\n","sentihood_aspects = [\"general\", \"price\", \"safety\", \"transit location\"]\n","sentihood_label2id = {\"none\": 0, \"positive\": 1, \"negative\": 2}"],"metadata":{"id":"krPqEOS9WlUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_sentihood_single(data, output_path, set):\n","    output = {}\n","    for location in sentihood_locations:\n","        output[location] = {}\n","        for aspect in sentihood_aspects:\n","            output[location][aspect] = [[\"id\", \"original_sentence\", \"aspect\", \"label_id\", \"label\"]]\n","    for entry in data:\n","        id = entry[\"id\"]\n","        original_sentence = entry[\"text\"]\n","        for location in sentihood_locations:\n","            if location in original_sentence:\n","                for aspect in sentihood_aspects:\n","                    label = \"none\"\n","                    for opinion in entry[\"opinions\"]:\n","                        if opinion[\"target_entity\"] == location and opinion[\"aspect\"] == aspect:\n","                            if opinion[\"sentiment\"] == \"Positive\":\n","                                label = \"positive\"\n","                            elif opinion[\"sentiment\"] == \"Negative\":\n","                                label = \"negative\"\n","                    output[location][aspect].append([id, original_sentence, aspect, sentihood_label2id[label], label])\n","    for location in sentihood_locations:\n","        for aspect in sentihood_aspects:\n","            output[location][aspect][1:] = sorted(output[location][aspect][1:], key=lambda el: el[0])\n","            df = pd.DataFrame(output[location][aspect])\n","            print(f\"{output_path}/location_{location[-1]}_{aspect}/{set}.csv\")\n","            df.to_csv(f\"{output_path}/location_{location[-1]}_{aspect}/{set}.csv\", sep='\\t', index=False, header=False)\n","\n","\n","def generate_semeval_NLI_M(data, output_path):\n","    output = []\n","    for entry in data:\n","        id = entry[\"id\"]\n","        original_sentence = entry[\"text\"]\n","        for location in sentihood_locations:\n","            if location in original_sentence:\n","                for aspect in sentihood_aspects:\n","                    auxiliary_sentence = f\"{location} - {aspect}\"\n","                    label = \"none\"\n","                    for opinion in entry[\"opinions\"]:\n","                        if opinion[\"target_entity\"] == location and opinion[\"aspect\"] == aspect:\n","                            if opinion[\"sentiment\"] == \"Positive\":\n","                                label = \"positive\"\n","                            elif opinion[\"sentiment\"] == \"Negative\":\n","                                label = \"negative\"\n","                    output.append([id, original_sentence, auxiliary_sentence, sentihood_label2id[label], label])\n","    loc1_rows = sorted([row for row in output if \"location - 1\" in row[2]], key=lambda el: el[0])\n","    loc2_rows = sorted([row for row in output if \"location - 2\" in row[2]], key=lambda el: el[0])\n","    output = [[\"id\", \"original_sentence\", \"auxiliary_sentence\", \"label_id\", \"label\"]]\n","    output += loc1_rows + loc2_rows\n","    df = pd.DataFrame(output)\n","    df.to_csv(output_path, sep='\\t', index=False, header=False)\n","\n","def convert_sentihood_input(data):\n","    for entry in data:\n","        entry[\"text\"] = entry[\"text\"].replace(\"LOCATION2\", \"location - 2\").replace(\"LOCATION1\", \"location - 1\")\n","        for opinion in entry[\"opinions\"]:\n","            opinion[\"target_entity\"] = opinion[\"target_entity\"]\\\n","                    .replace(\"LOCATION1\", \"location - 1\").replace(\"LOCATION2\", \"location - 2\")\n","            opinion[\"aspect\"] = opinion[\"aspect\"].replace(\"transit-location\", \"transit location\")\n","    return data"],"metadata":{"id":"YlYs_0FMWhAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SENTIHOOD_DIR = \"sentihood\"\n","\n","for location in sentihood_locations:\n","  print(location)\n","  for aspect in sentihood_aspects:\n","    print(aspect)\n","    Path(f\"{SENTIHOOD_DIR}/location_{location[-1]}_{aspect}\").mkdir(parents=True, exist_ok=True)"],"metadata":{"id":"uLN-1BLEYTYY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647186323694,"user_tz":-480,"elapsed":1,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"291c2347-7395-4d1c-c646-d874136e427d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["location - 1\n","general\n","price\n","safety\n","transit location\n","location - 2\n","general\n","price\n","safety\n","transit location\n"]}]},{"cell_type":"code","source":["for file in os.scandir(SENTIHOOD_DIR):\n","    if file.name.endswith(\".json\"):\n","        with open(file.path, \"r\") as f:\n","            print(file.path)\n","            data = json.loads(f.read())\n","            data = convert_sentihood_input(data)\n","            finds = re.findall(\"(?<=-).*(?=\\.)\", file.name)[0]\n","            output_path = f\"{SENTIHOOD_DIR}\"\n","            generate_sentihood_single(data, output_path, finds)\n","\n","            output_path = f\"{SENTIHOOD_DIR}/{finds}_NLI_M.csv\"\n","            generate_sentihood_NLI_M(data, output_path)"],"metadata":{"id":"x45DvH0xZBny","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647186791186,"user_tz":-480,"elapsed":1297,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"90f54ca0-fbdb-4c90-a560-aefd78d649da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sentihood/sentihood-dev.json\n","sentihood/location_1_general/dev.csv\n","sentihood/location_1_price/dev.csv\n","sentihood/location_1_safety/dev.csv\n","sentihood/location_1_transit location/dev.csv\n","sentihood/location_2_general/dev.csv\n","sentihood/location_2_price/dev.csv\n","sentihood/location_2_safety/dev.csv\n","sentihood/location_2_transit location/dev.csv\n","sentihood/sentihood-train.json\n","sentihood/location_1_general/train.csv\n","sentihood/location_1_price/train.csv\n","sentihood/location_1_safety/train.csv\n","sentihood/location_1_transit location/train.csv\n","sentihood/location_2_general/train.csv\n","sentihood/location_2_price/train.csv\n","sentihood/location_2_safety/train.csv\n","sentihood/location_2_transit location/train.csv\n","sentihood/sentihood-test.json\n","sentihood/location_1_general/test.csv\n","sentihood/location_1_price/test.csv\n","sentihood/location_1_safety/test.csv\n","sentihood/location_1_transit location/test.csv\n","sentihood/location_2_general/test.csv\n","sentihood/location_2_price/test.csv\n","sentihood/location_2_safety/test.csv\n","sentihood/location_2_transit location/test.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","\n","base_dir = \"sentihood\"\n","id2label = {0: \"None\", 1: \"Positive\", 2: \"Negative\"}\n","label2id = {\"None\": 0, \"Positive\": 1, \"Negative\": 2}\n","num_classes = 3\n","\n","locations = [\"location_2_\"]\n","aspects = [\"transit location\"]\n","\n","# locations = [\"location_1_\", \"location_2_\"]\n","# aspects = [\"general\", \"price\", \"safety\", \"transit location\"]\n","\n","\n","\n","def get_dataset(path):\n","    original_sentences = []\n","    labels = []\n","    data = pd.read_csv(path, header=0, sep=\"\\t\").values.tolist()\n","    for row in data:\n","        original_sentences.append(row[1])\n","        labels.append(row[3])\n","    return original_sentences, labels\n","\n","\n","train_original_sentences = {}\n","train_labels = {}\n","val_original_sentences = {}\n","val_labels = {}\n","test_original_sentences = {}\n","test_labels = {}\n","\n","for location in locations:\n","    train_original_sentences[location] = {}\n","    train_labels[location] = {}\n","    val_original_sentences[location] = {}\n","    val_labels[location] = {}\n","    test_original_sentences[location] = {}\n","    test_labels[location] = {}\n","    for aspect in aspects:\n","        train_original_sentences[location][aspect], train_labels[location][aspect] = get_dataset(f\"{base_dir}/{location}{aspect}/train.csv\")\n","        val_original_sentences[location][aspect], val_labels[location][aspect] = get_dataset(f\"{base_dir}/{location}{aspect}/dev.csv\")\n","        test_original_sentences[location][aspect], test_labels[location][aspect] = get_dataset(f\"{base_dir}/{location}{aspect}/test.csv\")"],"metadata":{"id":"G6ykwf4DakKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","train_encodings = {}\n","val_encodings = {}\n","test_encodings = {}\n","for location in locations:\n","    train_encodings[location] = {}\n","    val_encodings[location] = {}\n","    test_encodings[location] = {}\n","    for aspect in aspects:\n","        train_encodings[location][aspect] = tokenizer(train_original_sentences[location][aspect], truncation=True, padding=True)\n","        val_encodings[location][aspect] = tokenizer(val_original_sentences[location][aspect], truncation=True, padding=True)\n","        test_encodings[location][aspect] = tokenizer(test_original_sentences[location][aspect], truncation=True, padding=True)"],"metadata":{"id":"y3tR999fakMa","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["41fe21361dcb44019b1976372fe991dc","6efbbef5e7704fcfb2b44160083d87e2","14a76c8cd4cf42128fca8a6d48cc7ffd","d446b38465f349868edb26b12718422a","d49166bb96c24c43b21bad532366dd6a","1094b5eb6fe14f4690f7bf9a3e5a51af","326729eda67844ba85348cc91b187387","72e1c2d3da3440e9a26589ff53ded533","f0b53c5c50654579b6e9661a2ea0f21a","68cec03bb06741e09fe7087500985149","4c1ade475fb84c9fa11c8bd1f7d3891f","8d1ae913a039422e9e131df84908fe89","84701d01cc8c4cae9bee77d8a20baafb","e8d322e93f394af093463bc710622a71","aca63fda6e494e04a8fcfb8b7f74cd6e","48e4ff08e341486e95dae342e2fbdb4e","5e6535c8f9a8416f8cd9345f4c29c70d","61be7ae62426426690c59dcc9567b232","822cf05a0e9b4a8194ad2454077f10bd","d3701bd329444c5dbc43d785ff69a27c","18ac00da429044e0a90aafd78f8b22ea","c856627f38d94211b01809aab921b7f6","337c7b3d4bdf4b7189f85c1d7ed8b251","07bdc0abebe0408599daeddd84ebdfc2","e700b022034440bc853636a762b69135","190de1c83a2e45f5ae7b78ed66ab15aa","800e75ed1633400c8cca915b92a8d671","31881affbf374b8d83258dfcaefd2e4f","3127850acb2341698e1e28d16f387952","a542cc10e9d94fcb906eb08191c41a08","2cab162293054f248580f7dd372c93af","7eec74ff96d6410da31d0d958dfba523","75036ee99908401d9d48e339da9817ba"]},"executionInfo":{"status":"ok","timestamp":1647183297833,"user_tz":-480,"elapsed":11855,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"3476c994-68de-4fda-e5fe-d52bf811c0ba"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41fe21361dcb44019b1976372fe991dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d1ae913a039422e9e131df84908fe89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"337c7b3d4bdf4b7189f85c1d7ed8b251"}},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","\n","class ABSA_Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","\n","train_dataset = {}\n","val_dataset = {}\n","test_dataset = {}\n","for location in locations:\n","    train_dataset[location] = {}\n","    val_dataset[location] = {}\n","    test_dataset[location] = {}\n","    for aspect in aspects:\n","        train_dataset[location][aspect] = ABSA_Dataset(train_encodings[location][aspect], train_labels[location][aspect])\n","        val_dataset[location][aspect] = ABSA_Dataset(val_encodings[location][aspect], val_labels[location][aspect])\n","        test_dataset[location][aspect] = ABSA_Dataset(test_encodings[location][aspect], test_labels[location][aspect])"],"metadata":{"id":"yGwG7JHBakOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertConfig\n","from transformers import logging\n","import gc\n","import pandas as pd\n","import numpy as np\n","from scipy.special import softmax\n","\n","epochs = 4\n","batch_size = 24\n","\n","header = [\"predicted_label\"]\n","for label in label2id.keys():\n","    header.append(label)\n","\n","config = BertConfig.from_pretrained(\n","        'bert-base-uncased',\n","        architectures = ['BertForSequenceClassification'],\n","        hidden_size = 768,\n","        num_hidden_layers = 12,\n","        num_attention_heads = 12,\n","        hidden_dropout_prob = 0.1,\n","        num_labels = num_classes\n","    )    \n","\n","for location in locations:\n","    for aspect in aspects:\n","        num_steps = len(train_dataset[location][aspect]) * epochs // batch_size\n","        warmup_steps = num_steps // 10  # 10% of the training steps\n","        save_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n","\n","\n","        training_args = TrainingArguments(\n","            output_dir = f'{base_dir}/{location}{aspect}/',          \n","            num_train_epochs = epochs,              \n","            per_device_train_batch_size = batch_size,  \n","            per_device_eval_batch_size = batch_size,   \n","            warmup_steps = warmup_steps,   \n","            weight_decay = 0.01,               \n","            evaluation_strategy = 'epoch',\n","            learning_rate = 2e-5,\n","            save_steps = save_steps,\n","            seed=21\n","        )\n","\n","        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","\n","        trainer = Trainer(\n","            model=model,                         \n","            args=training_args,                  \n","            train_dataset=train_dataset[location][aspect],         \n","            eval_dataset=val_dataset[location][aspect]             \n","        )\n","\n","        trainer.train()\n","\n","        model.save_pretrained(f\"{base_dir}/{location}{aspect}/last_step\")\n","\n","        results = trainer.predict(test_dataset[location][aspect])\n","\n","        scores = [softmax(prediction) for prediction in results.predictions]\n","        predicted_labels = [np.argmax(x) for x in scores]\n","\n","        csv_output = np.insert(scores, 0, predicted_labels, axis=1)\n","        df = pd.DataFrame(csv_output)\n","        df[0] = df[0].astype(\"int\")\n","        df.to_csv(f\"{base_dir}/results/{location}{aspect}.csv\", index=False, header=header)\n","\n","        del training_args\n","        del model\n","        del trainer\n","        del results\n","        del scores\n","        del predicted_labels\n","        del csv_output\n","        del df\n","        gc.collect()"],"metadata":{"id":"dme9yi1JbxX9","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["333ae03995474910a00ea23908d6b6ba","2a5b085e68c249d794755a48934e80be","cd155cafd16c41edaf1e25a88cf5cdb8","e010e47a3eca4dd3bd2d371b0f7689a6","643456f8c053447ab1f38a53b8eb193d","d6e6211ba5cc44e9a986e87ff3bde006","e431666a9b6c48f9b62afc474de74f1f","8a9e245e573845bc82df498f41ed767b","db74d85e6bc14c1b8a5b672235342d2f","1c9bd190b92245bcb7278876e899528e","02a2408850bf4f66b356f5134dfe93e8"]},"executionInfo":{"status":"ok","timestamp":1647183602431,"user_tz":-480,"elapsed":304604,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"af266b84-7c24-49ab-9547-549b054cb086"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333ae03995474910a00ea23908d6b6ba"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 775\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 132\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [132/132 04:12, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.367725</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.339700</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.324151</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.316308</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to sentihood/location_2_transit location/checkpoint-32\n","Configuration saved in sentihood/location_2_transit location/checkpoint-32/config.json\n","Model weights saved in sentihood/location_2_transit location/checkpoint-32/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 190\n","  Batch size = 24\n","Saving model checkpoint to sentihood/location_2_transit location/checkpoint-64\n","Configuration saved in sentihood/location_2_transit location/checkpoint-64/config.json\n","Model weights saved in sentihood/location_2_transit location/checkpoint-64/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 190\n","  Batch size = 24\n","Saving model checkpoint to sentihood/location_2_transit location/checkpoint-96\n","Configuration saved in sentihood/location_2_transit location/checkpoint-96/config.json\n","Model weights saved in sentihood/location_2_transit location/checkpoint-96/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 190\n","  Batch size = 24\n","Saving model checkpoint to sentihood/location_2_transit location/checkpoint-128\n","Configuration saved in sentihood/location_2_transit location/checkpoint-128/config.json\n","Model weights saved in sentihood/location_2_transit location/checkpoint-128/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 190\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sentihood/location_2_transit location/last_step/config.json\n","Model weights saved in sentihood/location_2_transit location/last_step/pytorch_model.bin\n","***** Running Prediction *****\n","  Num examples = 388\n","  Batch size = 24\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [17/17 00:04]\n","    </div>\n","    "]},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn import metrics\n","import numpy as np\n","import argparse\n","\n","def get_dataset():\n","    original_sentences = []\n","    auxiliary_sentences = []\n","    labels = []\n","    data = pd.read_csv(f\"sentihood/test_NLI_M.csv\", header=0, sep=\"\\t\").values.tolist()\n","    for row in data:\n","        original_sentences.append(row[1])\n","        auxiliary_sentences.append(row[2])\n","        labels.append(row[3])\n","    return original_sentences, auxiliary_sentences, labels"],"metadata":{"id":"5athx0ZCbxZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_predictions():\n","    predicted_labels = []\n","    scores = []\n","\n","\n","    sentihood_locations = [\"location_1\", \"location_2\"]\n","    sentihood_aspects = [\"general\", \"price\", \"safety\", \"transit location\"]\n","    data = {}\n","    for location in sentihood_locations:\n","        data[location] = {}\n","        for aspect in sentihood_aspects:\n","            data[location][aspect] = pd.read_csv(f\"sentihood/results/{location}_{aspect}.csv\", header=0).values.tolist()\n","    for location in sentihood_locations:\n","        for i in range(len(data[location][sentihood_aspects[0]])):\n","            for aspect in sentihood_aspects:\n","                scores.append(data[location][aspect][i][1:])\n","                predicted_labels.append(int(data[location][aspect][i][0]))\n","    return predicted_labels, scores"],"metadata":{"id":"sgxWlaZy1wGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentihood_label2id = {\"none\": 0, \"positive\": 1, \"negative\": 2}\n","def compute_sentihood_aspect_strict_accuracy(test_labels, predicted_labels):\n","    correct_count = 0\n","    num_examples = len(test_labels) // 4\n","    for i in range(num_examples):\n","        if test_labels[i * 4] == predicted_labels[i * 4]\\\n","                and test_labels[i * 4 + 1] == predicted_labels[i * 4 + 1]\\\n","                and test_labels[i * 4 + 2] == predicted_labels[i * 4 + 2]\\\n","                and test_labels[i * 4 + 3] == predicted_labels[i * 4 + 3]:\n","            correct_count += 1\n","    return correct_count / num_examples\n","\n","\n","def compute_sentihood_aspect_macro_F1(test_labels, predicted_labels):\n","    total_precision = 0\n","    total_recall = 0\n","    num_examples = len(test_labels) // 4\n","    count_examples_with_sentiments = 0\n","    for i in range(num_examples):\n","        test_aspects = set()\n","        predicted_aspects = set()\n","        for j in range(4):\n","            if test_labels[i * 4 + j] != 0:\n","                test_aspects.add(j)\n","            if predicted_labels[i * 4 + j] != 0:\n","                predicted_aspects.add(j)\n","        if len(test_aspects) == 0:\n","            continue\n","        intersection = test_aspects.intersection(predicted_aspects)\n","        if len(intersection) > 0:\n","            precision = len(intersection) / len(predicted_aspects)\n","            recall = len(intersection) / len(test_aspects)\n","        else:\n","            precision = 0\n","            recall = 0\n","        total_precision += precision\n","        total_recall += recall\n","        count_examples_with_sentiments += 1\n","    ma_P = total_precision / count_examples_with_sentiments\n","    ma_R = total_recall / count_examples_with_sentiments\n","    return (2 * ma_P * ma_R) / (ma_P + ma_R)\n","\n","\n","def compute_sentihood_aspect_macro_AUC(test_labels, scores):\n","    aspects_test_labels = [[] for _ in range(4)]\n","    aspects_none_scores = [[] for _ in range(4)]\n","    for i in range(len(test_labels)):\n","        if test_labels[i] != 0:\n","            new_label = 0\n","        else:\n","            new_label = 1   # For metrics.roc_auc_score you need to use the score of the maximum label, so \"None\" : 1\n","        aspects_test_labels[i % 4].append(new_label)\n","        aspects_none_scores[i % 4].append(scores[i][0])\n","    aspect_AUC = []\n","    for i in range(4):\n","        aspect_AUC.append(metrics.roc_auc_score(aspects_test_labels[i], aspects_none_scores[i]))\n","    aspect_macro_AUC = np.mean(aspect_AUC)\n","    return aspect_macro_AUC\n","\n","\n","def compute_sentihood_sentiment_classification_metrics(test_labels, scores):\n","    \"\"\"Compute macro AUC and accuracy for sentiment classification ignoring \"None\" scores\"\"\"\n","    # Macro AUC\n","    sentiment_test_labels = [[] for _ in range(4)]  # One list for each aspect\n","    sentiment_negative_scores = [[] for _ in range(4)]\n","    sentiment_predicted_label = []\n","    sentiment_test_label = []   # One global list\n","    for i in range(len(test_labels)):\n","        if test_labels[i] != 0:\n","            new_test_label = test_labels[i] - 1  # \"Positive\": 0, \"Negative\": 1\n","            sentiment_test_label.append(new_test_label)\n","            new_negative_score = scores[i][2] / (scores[i][1] + scores[i][2])   # Prob. of \"Negative\" ignoring \"None\"\n","            if new_negative_score > 0.5:\n","                sentiment_predicted_label.append(1)\n","            else:\n","                sentiment_predicted_label.append(0)\n","            sentiment_test_labels[i % 4].append(new_test_label)\n","            sentiment_negative_scores[i % 4].append(new_negative_score)\n","    sentiment_AUC = []\n","    for i in range(4):\n","        sentiment_AUC.append(metrics.roc_auc_score(sentiment_test_labels[i], sentiment_negative_scores[i]))\n","    sentiment_macro_AUC = np.mean(sentiment_AUC)\n","\n","    # Accuracy\n","    sentiment_accuracy = metrics.accuracy_score(sentiment_test_label, sentiment_predicted_label)\n","\n","    return sentiment_macro_AUC, sentiment_accuracy"],"metadata":{"id":"mPgKC92b5JW0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_labels, scores = get_predictions()\n","test_original_sentences, test_auxiliary_sentences, test_labels = get_dataset()\n","\n","sentihood_aspect_strict_acc = compute_sentihood_aspect_strict_accuracy(test_labels, predicted_labels)\n","print(f\"Sentihood aspect strict accuracy: {sentihood_aspect_strict_acc}\")\n","sentihood_aspect_macro_F1 = compute_sentihood_aspect_macro_F1(test_labels, predicted_labels)\n","print(f\"Sentihood aspect macro F1: {sentihood_aspect_macro_F1}\")\n","sentihood_aspect_macro_AUC = compute_sentihood_aspect_macro_AUC(test_labels, scores)\n","print(f\"Sentihood aspect macro AUC: {sentihood_aspect_macro_AUC}\")\n","\n","sentihood_sentiment_macro_AUC, sentihood_sentiment_accuracy = compute_sentihood_sentiment_classification_metrics(\n","    test_labels, scores)\n","print(f\"Sentihood sentiment accuracy: {sentihood_sentiment_accuracy}\")\n","print(f\"Sentihood sentiment macro AUC: {sentihood_sentiment_macro_AUC}\")"],"metadata":{"id":"DY4vJqQ2bxcb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647186975790,"user_tz":-480,"elapsed":2,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"db12a99a-f4fb-4dda-d0c5-62408410df05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentihood aspect strict accuracy: 0.736562001064396\n","Sentihood aspect macro F1: 0.7946455119394706\n","Sentihood aspect macro AUC: 0.9554963119304407\n","Sentihood sentiment accuracy: 0.8519736842105263\n","Sentihood sentiment macro AUC: 0.8558175292591795\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"TqgbMMSzbxed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Q4Dx8p_jbxgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JsMUff9HbxiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"YGgb2Avvbxjn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"zTcywjribxk_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jKb2PEF8bxmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ErhzywyYuudl"},"outputs":[],"source":["data_dir = 'sameval2014/data'\n","semeval_sentiments = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","semeval_label2id = {\"positive\": 0, \"neutral\": 1, \"negative\": 2, \"conflict\": 3, \"none\": 4}\n","\n","for aspect in aspects:\n","  path = data_dir + '/' + aspect\n","\n","  os.makedirs(path, exist_ok=True)\n","\n","os.makedirs(data_dir + '/predictions', exist_ok = True)\n","\n","def aspects_data(xml_file_path, aspect):\n","  if aspect == 'anecdotes':\n","    tmp_aspect = 'anecdotes/miscellaneous'\n","  else:\n","    tmp_aspect = aspect\n","  output = []\n","  with open(xml_file_path, \"r\") as f:\n","    tree = ET.parse(f)\n","    root = tree.getroot()\n","    for sentence in root:\n","        id = sentence.attrib[\"id\"]\n","        text = sentence.find(\"text\").text\n","        label = \"none\"\n","        for opinion in sentence.find(\"aspectCategories\"):\n","          if opinion.attrib[\"category\"] == tmp_aspect:\n","            label = opinion.attrib[\"polarity\"]\n","        data = [id, text, aspect, semeval_label2id[label], label]\n","        output.append(data)\n","  return output"]},{"cell_type":"markdown","metadata":{"id":"XmG84sagEZmK"},"source":["### Save Train Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-X4u-TgtEchp"},"outputs":[],"source":["train_path = data_dir + '/Restaurants_Train.xml'\n","for aspect in aspects:\n","  data = aspects_data(train_path, aspect)\n","  # data = sorted(data, key=lambda el: el[0])\n","  df = pd.DataFrame(data, columns = [\"id\", \"text\", \"aspect\", \"label_id\", \"label\"])\n","  df.to_csv(\"{}/{}/train.csv\".format(data_dir, aspect), sep='\\t', index=False)"]},{"cell_type":"markdown","metadata":{"id":"cx4yZZPWEdnC"},"source":["### Save Test Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFOfHadAEgzb"},"outputs":[],"source":["test_path = data_dir + '/Restaurants_Test_Gold.xml'\n","for aspect in aspects:\n","  data = aspects_data(test_path, aspect)\n","  # data = sorted(data, key=lambda el: el[0])\n","  df = pd.DataFrame(data, columns = [\"id\", \"text\", \"aspect\", \"label_id\", \"label\"])\n","  df.to_csv(\"{}/{}/test.csv\".format(data_dir, aspect), sep='\\t', index=False)"]},{"cell_type":"markdown","metadata":{"id":"SmGjjxUZEiGZ"},"source":["## Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"ird-ygXdEnGg"},"source":["### Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bT2NxYiCYKr"},"outputs":[],"source":["def get_train_data(data_dir, aspect):\n","  train_df = pd.read_csv(\"{}/{}/train.csv\".format(data_dir, aspect),header=0, sep=\"\\t\")\n","\n","  train_sentences = train_df['text'].tolist()\n","  train_labels = train_df['label_id'].tolist()\n","  return train_sentences, train_labels\n","\n","def get_test_data(data_dir, aspect):\n","  test_df = pd.read_csv(\"{}/{}/test.csv\".format(data_dir, aspect),header=0, sep=\"\\t\")\n","\n","  test_sentences = test_df['text'].tolist()\n","  test_labels = test_df['label_id'].tolist()\n","  return test_sentences, test_labels"]},{"cell_type":"code","source":["for aspect in aspects:\n","  train_sentences, train_labels = get_train_data(data_dir, aspect)\n","  test_sentences, test_labels = get_test_data(data_dir, aspect)"],"metadata":{"id":"ETmrU_EkrxBj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEJuni3gFCAr"},"outputs":[],"source":["class Semeval_Data(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"markdown","source":["## Data Analysis"],"metadata":{"id":"LvRbJu-lpcMo"}},{"cell_type":"markdown","source":["### Number of sentences in each aspect"],"metadata":{"id":"15AuNzXZpgfd"}},{"cell_type":"code","source":["label_count = [[],[]]\n","for aspect in aspects:\n","  train_sentences, train_labels = get_train_data(data_dir, aspect)\n","  test_sentences, test_labels = get_test_data(data_dir, aspect)\n","\n","  train_count = 0\n","  test_count = 0\n","  for label in train_labels:\n","    if label != 4:\n","      train_count = train_count+1\n","  \n","  for label in test_labels:\n","    if label != 4:\n","      test_count = test_count+1\n","\n","  label_count[0].append(train_count)\n","  label_count[1].append(test_count)\n","  print(\"Number of train sentences for aspect {}: {}\".format(aspect, train_count))\n","  print(\"Number of test sentences for aspect {}: {}\".format(aspect, test_count))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvuGLObSpnw2","executionInfo":{"status":"ok","timestamp":1647098647247,"user_tz":-480,"elapsed":312,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"7f2607bf-fc36-4296-9232-1b0e69fa1928"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of train sentences for aspect price: 319\n","Number of test sentences for aspect price: 83\n","Number of train sentences for aspect anecdotes: 1131\n","Number of test sentences for aspect anecdotes: 234\n","Number of train sentences for aspect food: 1233\n","Number of test sentences for aspect food: 418\n","Number of train sentences for aspect ambience: 432\n","Number of test sentences for aspect ambience: 118\n","Number of train sentences for aspect service: 597\n","Number of test sentences for aspect service: 172\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n"," \n","# set width of bar\n","barWidth = 0.25\n","fig = plt.subplots(figsize =(12, 8))\n"," \n","# set height of bar\n","Train = label_count[0]\n","Test = label_count[1]\n"," \n","# Set position of bar on X axis\n","br1 = np.arange(len(Train))\n","br2 = [x + barWidth for x in br1]\n"," \n","# Make the plot\n","plt.bar(br1, Train, color ='b', width = barWidth,\n","        edgecolor ='grey', label ='Train')\n","plt.bar(br2, Test, color ='g', width = barWidth,\n","        edgecolor ='grey', label ='test')\n"," \n","# Adding Xticks\n","plt.xlabel('Aspects', fontsize = 10)\n","plt.ylabel('Count', fontsize = 10)\n","plt.xticks([r + barWidth for r in range(len(Train))], aspects)\n"," \n","plt.legend()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"id":"-xBFTLwxuLI6","executionInfo":{"status":"ok","timestamp":1647098648182,"user_tz":-480,"elapsed":632,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"3c77db90-5fe6-4a5b-897a-56baeca26d17"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x576 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ZixIDfP0FVNn"},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7c8accbc80f344e28403012bfdb60610","dca985180c2d4aca967f4059e18092ac","cb8949cee9244ad8a6e8855056873701","fad6f18e7d8141f28a655a99ebf9c4f8","9c4fc35e145348cd86b0c3ba69b41a44","25664f768c0444c0809d2dbc3c3c17d6","e2d2f4e0e1504ed4b4897ed488f7fb6d","0e8ea6ea8b2940c585b282326c1a5459","841cb4d3bc64494b84b65d967dacb07a","5b2cf497a91e439f94a1ee5bfabb146c","75f57d87b04d48109f0b05b72344cad0","37058808418e40b1b4064860f47ac2e4","fa053a87f3954ea3adc113671d8a3056","c2e45b330911469b8d5a1247443910cc","bca901c26edc46aa9fc4a9a3e64f4954","a1aac721be4e4971a500ff3571157b84","cd6d153297044bb297cb2486ae9287a1","58bfb06fa0c447b186184d5a90a25307","5e550ca73a8c420e808a787339f0feb3","3c34f2e19f9549f0b0f6a681cda16510","c832c9bfdd074fe997698aad74e354e6","dfe724827d7c40139f66f98276f49b59","7ad8655a428a4c449fab8c187d0b0038","f69bae6007ca4ef495727f5a0f28f106","94d50c5eb05a4e55b35537b21cf619a7","8ad49324eb0545e0a028ff90fd05bab9","94955c7220764f7cb39b2eca6d39aa10","ab1d117106384f7f9e2dfbe4cf80e55d","b09a7b04a9c444d9bac9bf96486362b8","f9a6026d63164faf9b69eb52516281c4","cd0d2302df66487ca435600a42786c96","13720550bafb4a4d96f1121aa6e2e30e","903e9d57b102487a8034291811c4785d","0b1edb2871ce44fc9803783c6ec6a502","91df3c63fe7e452987415a108542b392","b72522c30e20427f8c4b1f8b0466c50a","1f1a9c6efa294b72b22903a75cc105b7","be4ba6103eca470289f5bf02931e5e96","e1903a3500454dc39e923c9f971015df","2111fb98d1e647169c03915f7135d4d5","7b3a5966ce784bb4bcbeba778579100a","58b1cdd1bb044567b431749885bf4dae","1869a531648b46a6b846bf46dca3aac7","830ac025bab04d2083e39e946e21725e","f5a090387013433da6a968c2be1e6d9d","46014578a53b422ebe3ab6b7cae16a47","2b964e32ad264df48235a2def9401a96","983c1f4f4b204a75a3ecaef23414e615","05d8302a9b4e47e0afa2404f82f6637f","b70ff7addc8c49e8a87184cddc273d82","3e1e42a811ff4b8da0b95852dc7071fa","4fd0909b6a8848beb8de9d314dd2671c","edf5abe6149a49ddb3dbd9d27852a8f0","52ce9ea0c7ee4ed3a6e9726b1f57b835","a839048e592f472a96292c8f5c94f877"]},"executionInfo":{"elapsed":2098633,"status":"ok","timestamp":1640144512395,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"},"user_tz":-480},"id":"wUH-bnJN2ph-","outputId":"0c974454-50c3-4866-9bc8-01955c96a87c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c8accbc80f344e28403012bfdb60610","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37058808418e40b1b4064860f47ac2e4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ad8655a428a4c449fab8c187d0b0038","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b1edb2871ce44fc9803783c6ec6a502","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5a090387013433da6a968c2be1e6d9d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:43, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.207300</td>\n","      <td>0.137184</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.241900</td>\n","      <td>0.125384</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.123400</td>\n","      <td>0.105945</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.080900</td>\n","      <td>0.094190</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/price/checkpoint-500\n","Configuration saved in sameval2014/data/model/price/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/price/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/price/config.json\n","Model weights saved in sameval2014/data/model/price/pytorch_model.bin\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:43, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.763300</td>\n","      <td>0.579846</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.478500</td>\n","      <td>0.499453</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.351300</td>\n","      <td>0.485604</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.217900</td>\n","      <td>0.495365</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/anecdotes/checkpoint-500\n","Configuration saved in sameval2014/data/model/anecdotes/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/anecdotes/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/anecdotes/config.json\n","Model weights saved in sameval2014/data/model/anecdotes/pytorch_model.bin\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:43, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.499700</td>\n","      <td>0.509694</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.505300</td>\n","      <td>0.417509</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.271300</td>\n","      <td>0.439859</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.172100</td>\n","      <td>0.450723</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/food/checkpoint-500\n","Configuration saved in sameval2014/data/model/food/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/food/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/food/config.json\n","Model weights saved in sameval2014/data/model/food/pytorch_model.bin\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:43, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.345200</td>\n","      <td>0.280426</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.196400</td>\n","      <td>0.244750</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.121300</td>\n","      <td>0.218265</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.073900</td>\n","      <td>0.219018</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/ambience/checkpoint-500\n","Configuration saved in sameval2014/data/model/ambience/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/ambience/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/ambience/config.json\n","Model weights saved in sameval2014/data/model/ambience/pytorch_model.bin\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 3044\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [508/508 06:45, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.325800</td>\n","      <td>0.232341</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.184300</td>\n","      <td>0.167020</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.121500</td>\n","      <td>0.190611</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.068000</td>\n","      <td>0.183390</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","Saving model checkpoint to sameval2014/data/model/service/checkpoint-500\n","Configuration saved in sameval2014/data/model/service/checkpoint-500/config.json\n","Model weights saved in sameval2014/data/model/service/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 800\n","  Batch size = 24\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in sameval2014/data/model/service/config.json\n","Model weights saved in sameval2014/data/model/service/pytorch_model.bin\n"]}],"source":["data_dir = 'sameval2014/data'\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","labels = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","\n","import torch\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import Trainer\n","from transformers import TrainingArguments\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","epochs = 4\n","batch_size = 24\n","learning_rate = 2e-5\n","\n","for aspect in aspects:\n","  training_args = TrainingArguments(\n","      output_dir = \"{}/model/{}\".format(data_dir, aspect),          \n","      num_train_epochs = epochs,              \n","      per_device_train_batch_size = batch_size,  \n","      per_device_eval_batch_size = batch_size,    \n","      weight_decay = 0.01,               \n","      logging_dir = \"{}/logs/{}\".format(data_dir, aspect),            \n","      logging_steps = 10,\n","      evaluation_strategy = 'epoch',\n","      learning_rate = learning_rate,\n","      seed=40\n","  )\n","\n","  # Get Data\n","  train_sentences, train_labels = get_train_data(data_dir, aspect)\n","  test_sentences, test_labels = get_test_data(data_dir, aspect)\n","\n","  # Tokenize sentences\n","  train_sentences_tokenized = tokenizer(train_sentences, truncation=True, padding=True)\n","  test_sentences_tokenized = tokenizer(test_sentences, truncation=True, padding=True)\n","\n","  # Create training data\n","  train_data = Semeval_Data(train_sentences_tokenized, train_labels)\n","  test_data = Semeval_Data(test_sentences_tokenized, test_labels)\n","\n","  # Load model\n","  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels))\n","\n","  trainer = Trainer(\n","      model,\n","      training_args,\n","      train_dataset=train_data,\n","      eval_dataset=test_data,\n","  )\n","\n","  trainer.train()\n","\n","  # Save model\n","  model.save_pretrained(\"{}/model/{}\".format(data_dir, aspect))"]},{"cell_type":"markdown","metadata":{"id":"30ZnyX4PKDUl"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"aAp8RI9EKDYe"},"source":["## Make Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":59211,"status":"ok","timestamp":1640145051617,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"},"user_tz":-480},"id":"MqYsgy3keR9q","outputId":"7ed46eab-c997-4483-b46d-649ca77e76da"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file sameval2014/data/model/price/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/price\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/price/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/price.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:08]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file sameval2014/data/model/anecdotes/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/anecdotes\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/anecdotes/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/anecdotes.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:07]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file sameval2014/data/model/food/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/food\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/food/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/food.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:07]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file sameval2014/data/model/ambience/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/ambience\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/ambience/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/ambience.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:07]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file sameval2014/data/model/service/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/service\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.14.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/service/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/service.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 800\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:07]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from scipy.special import softmax\n","from transformers import AutoModelForSequenceClassification\n","from transformers import Trainer\n","from transformers import AutoTokenizer\n","\n","checkpoint = \"bert-base-uncased\"\n","data_dir = 'sameval2014/data'\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","labels = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","\n","for aspect in aspects:\n","  test_sentences, test_labels = get_test_data(data_dir, aspect)\n","\n","  # Tokenize sentences\n","  test_encodings = tokenizer(test_sentences, truncation=True, padding=True)\n","\n","  # Create training data\n","  test_dataset = Semeval_Data(test_encodings, test_labels)\n","\n","  # Load model\n","  model = AutoModelForSequenceClassification.from_pretrained(\"{}/model/{}\".format(data_dir, aspect))\n","  trainer = Trainer(model=model)\n","  trainer.model = model.cuda()\n","  predictions = trainer.predict(test_dataset)\n","\n","  probabilities = [softmax(prediction) for prediction in predictions.predictions]\n","  y_pred = [np.argmax(x) for x in probabilities]\n","\n","  df = pd.DataFrame()\n","  df['predicted_label'] = y_pred\n","  df = pd.concat([df,pd.DataFrame(probabilities, index=df.index, columns=labels)], axis=1)\n","  df.to_csv(\"{}/predictions/{}.csv\".format(data_dir, aspect), index=False)"]},{"cell_type":"markdown","metadata":{"id":"KyOLiUH4EMIh"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"SpwVIA655i3X"},"source":["## Results Evaluation"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","labels = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","data_dir = 'sameval2014/data'"],"metadata":{"id":"2dtBl7EptE76"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dKZ02pXX6SMv"},"source":["### Utility Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcJdx9ziO9dZ"},"outputs":[],"source":["def get_predictions(data_dir):\n","  print(\"Getting predicted labels from {}\".format(data_dir))\n","  predicted_labels = []\n","  scores = []\n","\n","  for aspect in aspects:\n","      df = pd.read_csv(\"{}/predictions/{}.csv\".format(data_dir, aspect), header=0)\n","      pred_labels = df['predicted_label'].values.tolist()\n","      score = df[labels].values.tolist()\n","      predicted_labels.extend(pred_labels)\n","      scores.extend(score)\n","  return predicted_labels, scores\n","\n","def get_true_labels(data_dir):\n","  print(\"Getting actual labels from {}\\n\".format(data_dir))\n","  test_labels = []\n","  for aspect in aspects:\n","      df = pd.read_csv(\"{}/{}/test.csv\".format(data_dir, aspect), header=0, sep=\"\\t\")\n","      test_label = df['label_id'].values.tolist()\n","      test_labels.extend(test_label)\n","\n","  return test_labels"]},{"cell_type":"markdown","metadata":{"id":"oDnSN31Y6U0e"},"source":["### Aspect Detection"]},{"cell_type":"code","source":["def aspect_detection_prf(test_labels, predicted_labels):\n","    true_postive = 0   # aspect present, predicted present\n","    false_postive = 0  # aspect not present, predicted present\n","    false_negative = 0  # aspect present, predicted not present\n","    true_negative = 0   # aspect not present, predicted not present\n","\n","    for i in range(len(predicted_labels)):\n","      if predicted_labels[i] == 4 and test_labels[i] == 4: # true negative\n","        true_negative += 1\n","      elif predicted_labels[i] == 4 and test_labels[i] != 4: # false negative\n","        false_negative += 1\n","      elif predicted_labels[i] != 4 and test_labels[i] == 4: # false postive\n","        false_postive += 1\n","      elif predicted_labels[i] != 4 and test_labels[i] != 4: # true postive\n","        true_postive += 1\n","\n","    precision = true_postive / (true_postive + false_postive)\n","    recall = true_postive / (true_postive + false_negative)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return  precision, recall, f1"],"metadata":{"id":"XC17LPN_5D_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aspect Detection Evaluation\n","predicted_labels, scores = get_predictions(data_dir)\n","test_labels = get_true_labels(data_dir)\n","\n","precision, recall, accuracy = aspect_detection_prf_hussain(test_labels,predicted_labels)\n","print(\"Semeval aspect precision: {}\".format(precision))\n","print(\"Semeval aspect recall: {}\".format(recall))\n","print(\"Semeval aspect F1: {}\".format(accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rto5OuKM7JRa","executionInfo":{"status":"ok","timestamp":1647102335948,"user_tz":-480,"elapsed":1,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"dd6fbe65-af48-4967-ed28-df6aae8bd068"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Getting predicted labels from sameval2014/data\n","Getting actual labels from sameval2014/data\n","\n","Semeval aspect precision: 0.9303991811668373\n","Semeval aspect recall: 0.8868292682926829\n","Semeval aspect F1: 0.908091908091908\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5yOIKDRZzVwn"},"outputs":[],"source":["# def aspect_detection_prf(test_labels, predicted_labels):\n","#     num_total_intersection = 0\n","#     num_total_test_aspects = 0\n","#     num_total_predicted_aspects = 0\n","#     num_examples = len(test_labels) // 5\n","#     for i in range(num_examples):\n","#         test_aspects = set()\n","#         predicted_aspects = set()\n","#         for j in range(5):\n","#             if test_labels[i * 5 + j] != 4:\n","#                 test_aspects.add(j)\n","#             if predicted_labels[i * 5 + j] != 4:\n","#                 predicted_aspects.add(j)\n","#         if len(test_aspects) == 0:\n","#             continue\n","#         intersection = test_aspects.intersection(predicted_aspects)\n","#         num_total_test_aspects += len(test_aspects)\n","#         num_total_predicted_aspects += len(predicted_aspects)\n","#         num_total_intersection += len(intersection)\n","#     precision = num_total_intersection / num_total_predicted_aspects\n","#     recall = num_total_intersection / num_total_test_aspects\n","#     micro_F1 = (2 * precision * recall) / (precision + recall)\n","#     return  precision, recall, micro_F1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rD1pdm8Y8nC"},"outputs":[],"source":["# # Aspect Detection Evaluation\n","# predicted_labels, scores = get_predictions(data_dir)\n","# test_labels = get_true_labels(data_dir)\n","\n","# precision, recall, micro_F1 = aspect_detection_prf(test_labels,predicted_labels)\n","# print(\"Semeval aspect precision: {}\".format(precision))\n","# print(\"Semeval aspect recall: {}\".format(recall))\n","# print(\"Semeval aspect micro F1: {}\".format(micro_F1))"]},{"cell_type":"markdown","metadata":{"id":"gJRXfQdY6XZX"},"source":["### Sentiment Polarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xl7tFKYZ5m-d"},"outputs":[],"source":["def get_n_way_accuracy(test_labels, predicted_labels, scores, labels_range):\n","  no_examples = 0\n","  no_true_examples = 0\n","  for i in range(len(test_labels)):\n","    if test_labels[i] in labels_range: # inside the labels range\n","      temp_label = predicted_labels[i] # Extract the label\n","\n","      # If predicted label is not in labels range, compute new predicted label after making the score 0 for labels not in labels range\n","      if temp_label not in labels_range:\n","          new_scores = [0 if index not in labels_range else scores[i][index] for index in range(len(scores[i]))]\n","          temp_label = np.argmax(new_scores)\n","            \n","\n","      if test_labels[i] == temp_label:\n","          no_true_examples += 1\n","      no_examples += 1\n","    \n","  accuracy = no_true_examples / no_examples\n","  return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1647102339685,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"},"user_tz":-480},"id":"DMIen7ve6LBx","outputId":"567721da-4f8e-46fb-f174-8cb62a0683a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Getting predicted labels from sameval2014/data\n","Getting actual labels from sameval2014/data\n","\n"]}],"source":["predicted_labels, scores = get_predictions(data_dir)\n","test_labels = get_true_labels(data_dir)\n","binary_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,2))\n","three_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2))\n","four_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1647102339685,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"},"user_tz":-480},"id":"50oTvqq07k-y","outputId":"deb10c2e-6ba4-4526-de97-c1ed393f2a0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Binary Accuracy: 0.9271899886234357\n","Three Way Accuracy: 0.8612538540596094\n","Four Way Accuracy: 0.8214634146341463\n"]}],"source":["print(\"Binary Accuracy: {}\".format(binary_accuracy))\n","print(\"Three Way Accuracy: {}\".format(three_accuracy))\n","print(\"Four Way Accuracy: {}\".format(four_accuracy))"]},{"cell_type":"markdown","source":["### Aspect level evaluation"],"metadata":{"id":"7-SWS0oU8npM"}},{"cell_type":"code","source":["def get_predictions_aspect(data_dir, aspect):\n","  #print(\"Getting predicted labels from {}/{}\".format(data_dir, aspect))\n","  predicted_labels = []\n","  scores = []\n","\n","  df = pd.read_csv(\"{}/predictions/{}.csv\".format(data_dir, aspect), header=0)\n","  predicted_labels = df['predicted_label'].values.tolist()\n","  scores = df[labels].values.tolist()\n","  return predicted_labels, scores\n","\n","def get_true_labels_aspect(data_dir, aspect):\n","  #print(\"Getting actual labels from {}/{}\\n\".format(data_dir, aspect))\n","\n","  df = pd.read_csv(\"{}/{}/test.csv\".format(data_dir, aspect), header=0, sep=\"\\t\")\n","  test_labels = df['label_id'].values.tolist()\n","\n","  return test_labels\n","\n","def aspect_detection_evaluation_aspect(data_dir, aspect):\n","    predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","    test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","    precision, recall, micro_F1 = aspect_detection_prf(test_labels,predicted_labels)\n","    print(\"Semeval aspect precision: {}\".format(precision))\n","    print(\"Semeval aspect recall: {}\".format(recall))\n","    print(\"Semeval aspect micro F1: {}\".format(micro_F1))\n","    return"],"metadata":{"id":"4eIn3VIsRHHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aspect Detection Evaluation per aspect\n","data_dir = \"sameval2014/data\"\n","evaluation_aspect_detection = {'Precision': [], 'Recall': [], 'F1': []}\n","evaluation_sentiment_polarity = {'Binary Accuracy': [], 'Three Way Accuracy': [], 'Four Way Accuracy': []}\n","for aspect in aspects:\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  # Aspect detection\n","  precision, recall, f1 = aspect_detection_prf(test_labels,predicted_labels)\n","  evaluation_aspect_detection['Precision'].append(precision)\n","  evaluation_aspect_detection['Recall'].append(recall)\n","  evaluation_aspect_detection['F1'].append(f1)\n","\n","  # Sentiment Polarity\n","  binary_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,2))\n","  three_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2))\n","  four_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2,3))\n","  evaluation_sentiment_polarity['Binary Accuracy'].append(binary_accuracy)\n","  evaluation_sentiment_polarity['Three Way Accuracy'].append(three_accuracy)\n","  evaluation_sentiment_polarity['Four Way Accuracy'].append(four_accuracy)\n","\n","df_aspect_detection = pd.DataFrame(data=evaluation_aspect_detection, index=aspects)\n","df_sentiment_polarity = pd.DataFrame(data=evaluation_sentiment_polarity, index=aspects)\n","\n","print(\"Aspect Detection\")\n","print(df_aspect_detection) \n","\n","print(\"\\nSentiment Polarity\")\n","print(df_sentiment_polarity)  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5SvpkO5D-qTM","executionInfo":{"status":"ok","timestamp":1647102364670,"user_tz":-480,"elapsed":318,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"2e26e648-98c1-497f-dc67-359b828a49b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Aspect Detection\n","           Precision    Recall        F1\n","price       0.987179  0.927711  0.956522\n","anecdotes   0.894472  0.760684  0.822171\n","food        0.955990  0.935407  0.945586\n","ambience    0.896552  0.881356  0.888889\n","service     0.908571  0.924419  0.916427\n","\n","Sentiment Polarity\n","           Binary Accuracy  Three Way Accuracy  Four Way Accuracy\n","price             0.848101            0.837500           0.807229\n","anecdotes         0.922619            0.799087           0.747863\n","food              0.938005            0.873134           0.846890\n","ambience          0.938144            0.866667           0.779661\n","service           0.939024            0.922156           0.895349\n"]}]},{"cell_type":"code","source":["# Aspect Detection Evaluation per aspect\n","data_dir = \"sameval2014/data\"\n","evaluation_aspect_detection = {'Precision': [], 'Recall': [], 'Micro F1': []}\n","evaluation_sentiment_polarity = {'Binary Accuracy': [], 'Three Way Accuracy': [], 'Four Way Accuracy': []}\n","for aspect in aspects:\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  # Aspect detection\n","  precision, recall, micro_F1 = aspect_detection_prf(test_labels,predicted_labels)\n","  evaluation_aspect_detection['Precision'].append(precision)\n","  evaluation_aspect_detection['Recall'].append(recall)\n","  evaluation_aspect_detection['Micro F1'].append(micro_F1)\n","\n","  # Sentiment Polarity\n","  binary_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,2))\n","  three_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2))\n","  four_accuracy = get_n_way_accuracy(test_labels, predicted_labels, scores, (0,1,2,3))\n","  evaluation_sentiment_polarity['Binary Accuracy'].append(binary_accuracy)\n","  evaluation_sentiment_polarity['Three Way Accuracy'].append(three_accuracy)\n","  evaluation_sentiment_polarity['Four Way Accuracy'].append(four_accuracy)\n","\n","df_aspect_detection = pd.DataFrame(data=evaluation_aspect_detection, index=aspects)\n","df_sentiment_polarity = pd.DataFrame(data=evaluation_sentiment_polarity, index=aspects)\n","\n","print(\"Aspect Detection\")\n","print(df_aspect_detection) \n","\n","print(\"\\nSentiment Polarity\")\n","print(df_sentiment_polarity)  "],"metadata":{"id":"iIVXrBM37hO4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Further Exploration"],"metadata":{"id":"qC_NQEfpOVOX"}},{"cell_type":"code","source":["sentences = pd.read_csv(\"{}/price/test.csv\".format(data_dir),header=0, sep=\"\\t\")['text'].tolist()\n","len(sentences)"],"metadata":{"id":"kUfc7LCghpdy","executionInfo":{"status":"ok","timestamp":1647095889530,"user_tz":-480,"elapsed":299,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec2420ca-f549-4fc0-80eb-5e2fd84adcd1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["800"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["#### Aspect labels detected when not present (false postive)"],"metadata":{"id":"9n1MhriL0cj-"}},{"cell_type":"code","source":["incorrect_aspect_detection = {\"Positive\":[], \"Negative\":[], \"Neutral\":[]}\n","\n","for aspect in aspects:\n","  print(\"Evaluation for {}\".format(aspect))\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  pos = 0\n","  neg = 0\n","  neutral = 0\n","  for i in range(len(predicted_labels)):\n","    predicted_label = predicted_labels[i]\n","    if predicted_label != 4 and test_labels[i] == 4:\n","      if predicted_label == 0:\n","        pos = pos + 1\n","      elif predicted_label == 1:\n","        neutral = neutral + 1\n","      elif predicted_label == 2:\n","        neg = neg + 1\n","\n","  incorrect_aspect_detection['Positive'].append(pos)\n","  incorrect_aspect_detection['Negative'].append(neg)\n","  incorrect_aspect_detection['Neutral'].append(neutral)\n","\n","print(pd.DataFrame(data=incorrect_aspect_detection, index=aspects))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_HWEacznKhX","executionInfo":{"status":"ok","timestamp":1647099446294,"user_tz":-480,"elapsed":331,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"953cbe1e-c574-4a79-8c4e-30ec1b11d127"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation for price\n","Evaluation for anecdotes\n","Evaluation for food\n","Evaluation for ambience\n","Evaluation for service\n","           Positive  Negative  Neutral\n","price             1         0        0\n","anecdotes        12         3        6\n","food             14         4        0\n","ambience         10         2        0\n","service           8         8        0\n"]}]},{"cell_type":"markdown","source":["#### Sentences of aspect detected when not present"],"metadata":{"id":"sYDibA0tyKy9"}},{"cell_type":"code","source":["incorrect_aspect_detection_sentences = {\"price\":[], \"anecdotes\":[], \"food\":[], \"ambience\":[], \"service\":[]}\n","for aspect in aspects:\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  for i in range(len(predicted_labels)):\n","    predicted_label = predicted_labels[i]\n","    if predicted_label != 4 and test_labels[i] == 4:\n","      incorrect_aspect_detection_sentences[aspect].append(sentences[i])\n","\n","incorrect_aspect_detection_sentences['price']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bt6D2WL_oXdj","executionInfo":{"status":"ok","timestamp":1647100018016,"user_tz":-480,"elapsed":316,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"12b9f59a-f705-4b50-b74d-54b0f7c3088e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['personally i say save your money.']"]},"metadata":{},"execution_count":80}]},{"cell_type":"markdown","source":["#### Aspect labels not detected when present (false negative)"],"metadata":{"id":"YdoY10j22HJB"}},{"cell_type":"code","source":["incorrect_aspect_detection = {\"Count\":[]}\n","\n","for aspect in aspects:\n","  print(\"Evaluation for {}\".format(aspect))\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  count = 0\n","  for i in range(len(predicted_labels)):\n","    predicted_label = predicted_labels[i]\n","    if predicted_label == 4 and test_labels[i] != 4:\n","      count += 1\n","  incorrect_aspect_detection['Count'].append(count)\n","\n","print(pd.DataFrame(data=incorrect_aspect_detection, index=aspects))\n"],"metadata":{"id":"abLmMl5GnKqc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647102461445,"user_tz":-480,"elapsed":300,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"7de5198c-cce8-4254-ec52-539a91ea236d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation for price\n","Evaluation for anecdotes\n","Evaluation for food\n","Evaluation for ambience\n","Evaluation for service\n","           Count\n","price          6\n","anecdotes     56\n","food          27\n","ambience      14\n","service       13\n"]}]},{"cell_type":"markdown","source":["#### Sentences of aspect not detected when present"],"metadata":{"id":"oageDDz49Txt"}},{"cell_type":"code","source":["incorrect_aspect_detection_sentences = {\"price\":[], \"anecdotes\":[], \"food\":[], \"ambience\":[], \"service\":[]}\n","for aspect in aspects:\n","  predicted_labels, scores = get_predictions_aspect(data_dir, aspect)\n","  test_labels = get_true_labels_aspect(data_dir, aspect)\n","\n","  for i in range(len(predicted_labels)):\n","    predicted_label = predicted_labels[i]\n","    if predicted_label == 4 and test_labels[i] != 4:\n","      incorrect_aspect_detection_sentences[aspect].append(sentences[i])\n","\n","incorrect_aspect_detection_sentences['service']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lb952In4-Pxv","executionInfo":{"status":"ok","timestamp":1647103110519,"user_tz":-480,"elapsed":301,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"93154f08-1100-4946-a92a-d6fe8fc47695"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"While there's a decent menu, it shouldn't take ten minutes to get your drinks and 45 for a dessert pizza.\",\n"," \"It's about $7 for lunch and they have take-out or dine-in.\",\n"," 'My order is on my table even on a busy friday night within 10 minutes (at the most) of hanging up the phone.',\n"," 'The food is so good and so popular that waiting can really be a nightmare.',\n"," 'Not to sound too negative but be wary of the delivary.',\n"," 'I found the food to be just as good as its owner, Da Silvano, just much less expensive.',\n"," \"It's no-fuss, fast and delicious.\",\n"," 'We ordered a glass of wine and were finished eating and paying before the wine came.',\n"," 'the restaurant was completely empty, but she gave me a dirty look and asked, no reservations?',\n"," 'Great food, good wine and an excellent host.',\n"," \"Don't ever bother - the drinks were awful, but it was the people who work there that really made this the worst experience at dining.\",\n"," 'Anyway, the owner was fake.',\n"," 'Owner is pleasant and entertaining.']"]},"metadata":{},"execution_count":119}]},{"cell_type":"markdown","source":["## Prediction Pipeline"],"metadata":{"id":"JkKAavTpE8qU"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import Trainer\n","from scipy.special import softmax\n","\n","\n","data_dir = 'sameval2014/data'\n","semeval_sentiments = [\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]\n","aspects = [\"price\", \"anecdotes\", \"food\", \"ambience\", \"service\"]\n","semeval_label2id = {\"positive\": 0, \"neutral\": 1, \"negative\": 2, \"conflict\": 3, \"none\": 4}\n","checkpoint = \"bert-base-uncased\"\n","\n","# Load models\n","models = []\n","for aspect in aspects: \n","  models.append(AutoModelForSequenceClassification.from_pretrained(\"{}/model/{}\".format(data_dir, aspect)))\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nOnlJngSFQ88","executionInfo":{"status":"ok","timestamp":1645958791303,"user_tz":-480,"elapsed":20468,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"70be57dd-9607-437b-d59d-73fed875f4dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file sameval2014/data/model/price/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/price\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/price/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/price.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file sameval2014/data/model/anecdotes/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/anecdotes\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/anecdotes/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/anecdotes.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file sameval2014/data/model/food/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/food\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/food/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/food.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file sameval2014/data/model/ambience/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/ambience\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/ambience/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/ambience.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file sameval2014/data/model/service/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sameval2014/data/model/service\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file sameval2014/data/model/service/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sameval2014/data/model/service.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"code","source":["text = \"The food was amazing but it was abit expensive\"\n","\n","# tokenize sentence\n","test_encodings = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n","\n","def predict(models, test_encodings, aspects):\n","    sentiments = []\n","\n","    # Make predictions\n","    for i in range(len(models)):\n","      outputs = models[i](**test_encodings)\n","      probabilities = softmax(outputs.logits[0].detach())\n","      label = int(np.argmax(probabilities))\n","      sentiment = semeval_sentiments[label]\n","      sentiments.append(sentiment)\n","\n","    return sentiments\n","  \n","sentiments = predict(models, test_encodings, aspects)\n","\n","print(\"Aspects\")              \n","for i in range(len(sentiments)):\n","  if sentiments[i] != 'none':\n","    aspect = aspects[i]\n","    print('({}, {})'.format(aspect, sentiments[i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5KUs5M_KnNSp","executionInfo":{"status":"ok","timestamp":1645958943987,"user_tz":-480,"elapsed":1913,"user":{"displayName":"Hussain FYP","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06924807435667354264"}},"outputId":"41f55cd8-315b-4378-8833-ebe25ad16c1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Aspects\n","(price, negative)\n","(food, positive)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"23l18GNswL5w"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Aux sentences sentihood","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP5HYDYexH/CdngmKXLAAgF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"05d8302a9b4e47e0afa2404f82f6637f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b1edb2871ce44fc9803783c6ec6a502":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91df3c63fe7e452987415a108542b392","IPY_MODEL_b72522c30e20427f8c4b1f8b0466c50a","IPY_MODEL_1f1a9c6efa294b72b22903a75cc105b7"],"layout":"IPY_MODEL_be4ba6103eca470289f5bf02931e5e96"}},"0e8ea6ea8b2940c585b282326c1a5459":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13720550bafb4a4d96f1121aa6e2e30e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1869a531648b46a6b846bf46dca3aac7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f1a9c6efa294b72b22903a75cc105b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1869a531648b46a6b846bf46dca3aac7","placeholder":"​","style":"IPY_MODEL_830ac025bab04d2083e39e946e21725e","value":" 455k/455k [00:00&lt;00:00, 1.19MB/s]"}},"2111fb98d1e647169c03915f7135d4d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25664f768c0444c0809d2dbc3c3c17d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b964e32ad264df48235a2def9401a96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fd0909b6a8848beb8de9d314dd2671c","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_edf5abe6149a49ddb3dbd9d27852a8f0","value":440473133}},"37058808418e40b1b4064860f47ac2e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa053a87f3954ea3adc113671d8a3056","IPY_MODEL_c2e45b330911469b8d5a1247443910cc","IPY_MODEL_bca901c26edc46aa9fc4a9a3e64f4954"],"layout":"IPY_MODEL_a1aac721be4e4971a500ff3571157b84"}},"3c34f2e19f9549f0b0f6a681cda16510":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e1e42a811ff4b8da0b95852dc7071fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46014578a53b422ebe3ab6b7cae16a47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b70ff7addc8c49e8a87184cddc273d82","placeholder":"​","style":"IPY_MODEL_3e1e42a811ff4b8da0b95852dc7071fa","value":"Downloading: 100%"}},"4fd0909b6a8848beb8de9d314dd2671c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52ce9ea0c7ee4ed3a6e9726b1f57b835":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58b1cdd1bb044567b431749885bf4dae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58bfb06fa0c447b186184d5a90a25307":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b2cf497a91e439f94a1ee5bfabb146c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e550ca73a8c420e808a787339f0feb3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75f57d87b04d48109f0b05b72344cad0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ad8655a428a4c449fab8c187d0b0038":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f69bae6007ca4ef495727f5a0f28f106","IPY_MODEL_94d50c5eb05a4e55b35537b21cf619a7","IPY_MODEL_8ad49324eb0545e0a028ff90fd05bab9"],"layout":"IPY_MODEL_94955c7220764f7cb39b2eca6d39aa10"}},"7b3a5966ce784bb4bcbeba778579100a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c8accbc80f344e28403012bfdb60610":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dca985180c2d4aca967f4059e18092ac","IPY_MODEL_cb8949cee9244ad8a6e8855056873701","IPY_MODEL_fad6f18e7d8141f28a655a99ebf9c4f8"],"layout":"IPY_MODEL_9c4fc35e145348cd86b0c3ba69b41a44"}},"830ac025bab04d2083e39e946e21725e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"841cb4d3bc64494b84b65d967dacb07a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ad49324eb0545e0a028ff90fd05bab9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13720550bafb4a4d96f1121aa6e2e30e","placeholder":"​","style":"IPY_MODEL_903e9d57b102487a8034291811c4785d","value":" 226k/226k [00:00&lt;00:00, 5.47kB/s]"}},"903e9d57b102487a8034291811c4785d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91df3c63fe7e452987415a108542b392":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1903a3500454dc39e923c9f971015df","placeholder":"​","style":"IPY_MODEL_2111fb98d1e647169c03915f7135d4d5","value":"Downloading: 100%"}},"94955c7220764f7cb39b2eca6d39aa10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94d50c5eb05a4e55b35537b21cf619a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9a6026d63164faf9b69eb52516281c4","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd0d2302df66487ca435600a42786c96","value":231508}},"983c1f4f4b204a75a3ecaef23414e615":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52ce9ea0c7ee4ed3a6e9726b1f57b835","placeholder":"​","style":"IPY_MODEL_a839048e592f472a96292c8f5c94f877","value":" 420M/420M [00:21&lt;00:00, 19.9MB/s]"}},"9c4fc35e145348cd86b0c3ba69b41a44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1aac721be4e4971a500ff3571157b84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a839048e592f472a96292c8f5c94f877":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab1d117106384f7f9e2dfbe4cf80e55d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b09a7b04a9c444d9bac9bf96486362b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b70ff7addc8c49e8a87184cddc273d82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b72522c30e20427f8c4b1f8b0466c50a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b3a5966ce784bb4bcbeba778579100a","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58b1cdd1bb044567b431749885bf4dae","value":466062}},"bca901c26edc46aa9fc4a9a3e64f4954":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c832c9bfdd074fe997698aad74e354e6","placeholder":"​","style":"IPY_MODEL_dfe724827d7c40139f66f98276f49b59","value":" 570/570 [00:00&lt;00:00, 2.86kB/s]"}},"be4ba6103eca470289f5bf02931e5e96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2e45b330911469b8d5a1247443910cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e550ca73a8c420e808a787339f0feb3","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c34f2e19f9549f0b0f6a681cda16510","value":570}},"c832c9bfdd074fe997698aad74e354e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb8949cee9244ad8a6e8855056873701":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e8ea6ea8b2940c585b282326c1a5459","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_841cb4d3bc64494b84b65d967dacb07a","value":28}},"cd0d2302df66487ca435600a42786c96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd6d153297044bb297cb2486ae9287a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dca985180c2d4aca967f4059e18092ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25664f768c0444c0809d2dbc3c3c17d6","placeholder":"​","style":"IPY_MODEL_e2d2f4e0e1504ed4b4897ed488f7fb6d","value":"Downloading: 100%"}},"dfe724827d7c40139f66f98276f49b59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1903a3500454dc39e923c9f971015df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2d2f4e0e1504ed4b4897ed488f7fb6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edf5abe6149a49ddb3dbd9d27852a8f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5a090387013433da6a968c2be1e6d9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46014578a53b422ebe3ab6b7cae16a47","IPY_MODEL_2b964e32ad264df48235a2def9401a96","IPY_MODEL_983c1f4f4b204a75a3ecaef23414e615"],"layout":"IPY_MODEL_05d8302a9b4e47e0afa2404f82f6637f"}},"f69bae6007ca4ef495727f5a0f28f106":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab1d117106384f7f9e2dfbe4cf80e55d","placeholder":"​","style":"IPY_MODEL_b09a7b04a9c444d9bac9bf96486362b8","value":"Downloading: 100%"}},"f9a6026d63164faf9b69eb52516281c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa053a87f3954ea3adc113671d8a3056":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd6d153297044bb297cb2486ae9287a1","placeholder":"​","style":"IPY_MODEL_58bfb06fa0c447b186184d5a90a25307","value":"Downloading: 100%"}},"fad6f18e7d8141f28a655a99ebf9c4f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b2cf497a91e439f94a1ee5bfabb146c","placeholder":"​","style":"IPY_MODEL_75f57d87b04d48109f0b05b72344cad0","value":" 28.0/28.0 [00:00&lt;00:00, 213B/s]"}},"41fe21361dcb44019b1976372fe991dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6efbbef5e7704fcfb2b44160083d87e2","IPY_MODEL_14a76c8cd4cf42128fca8a6d48cc7ffd","IPY_MODEL_d446b38465f349868edb26b12718422a"],"layout":"IPY_MODEL_d49166bb96c24c43b21bad532366dd6a"}},"6efbbef5e7704fcfb2b44160083d87e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1094b5eb6fe14f4690f7bf9a3e5a51af","placeholder":"​","style":"IPY_MODEL_326729eda67844ba85348cc91b187387","value":"Downloading: 100%"}},"14a76c8cd4cf42128fca8a6d48cc7ffd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72e1c2d3da3440e9a26589ff53ded533","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0b53c5c50654579b6e9661a2ea0f21a","value":231508}},"d446b38465f349868edb26b12718422a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68cec03bb06741e09fe7087500985149","placeholder":"​","style":"IPY_MODEL_4c1ade475fb84c9fa11c8bd1f7d3891f","value":" 226k/226k [00:00&lt;00:00, 610kB/s]"}},"d49166bb96c24c43b21bad532366dd6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1094b5eb6fe14f4690f7bf9a3e5a51af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"326729eda67844ba85348cc91b187387":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72e1c2d3da3440e9a26589ff53ded533":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0b53c5c50654579b6e9661a2ea0f21a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68cec03bb06741e09fe7087500985149":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c1ade475fb84c9fa11c8bd1f7d3891f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d1ae913a039422e9e131df84908fe89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84701d01cc8c4cae9bee77d8a20baafb","IPY_MODEL_e8d322e93f394af093463bc710622a71","IPY_MODEL_aca63fda6e494e04a8fcfb8b7f74cd6e"],"layout":"IPY_MODEL_48e4ff08e341486e95dae342e2fbdb4e"}},"84701d01cc8c4cae9bee77d8a20baafb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e6535c8f9a8416f8cd9345f4c29c70d","placeholder":"​","style":"IPY_MODEL_61be7ae62426426690c59dcc9567b232","value":"Downloading: 100%"}},"e8d322e93f394af093463bc710622a71":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_822cf05a0e9b4a8194ad2454077f10bd","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3701bd329444c5dbc43d785ff69a27c","value":28}},"aca63fda6e494e04a8fcfb8b7f74cd6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18ac00da429044e0a90aafd78f8b22ea","placeholder":"​","style":"IPY_MODEL_c856627f38d94211b01809aab921b7f6","value":" 28.0/28.0 [00:00&lt;00:00, 221B/s]"}},"48e4ff08e341486e95dae342e2fbdb4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e6535c8f9a8416f8cd9345f4c29c70d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61be7ae62426426690c59dcc9567b232":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"822cf05a0e9b4a8194ad2454077f10bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3701bd329444c5dbc43d785ff69a27c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18ac00da429044e0a90aafd78f8b22ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c856627f38d94211b01809aab921b7f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"337c7b3d4bdf4b7189f85c1d7ed8b251":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07bdc0abebe0408599daeddd84ebdfc2","IPY_MODEL_e700b022034440bc853636a762b69135","IPY_MODEL_190de1c83a2e45f5ae7b78ed66ab15aa"],"layout":"IPY_MODEL_800e75ed1633400c8cca915b92a8d671"}},"07bdc0abebe0408599daeddd84ebdfc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31881affbf374b8d83258dfcaefd2e4f","placeholder":"​","style":"IPY_MODEL_3127850acb2341698e1e28d16f387952","value":"Downloading: 100%"}},"e700b022034440bc853636a762b69135":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a542cc10e9d94fcb906eb08191c41a08","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2cab162293054f248580f7dd372c93af","value":570}},"190de1c83a2e45f5ae7b78ed66ab15aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eec74ff96d6410da31d0d958dfba523","placeholder":"​","style":"IPY_MODEL_75036ee99908401d9d48e339da9817ba","value":" 570/570 [00:00&lt;00:00, 4.87kB/s]"}},"800e75ed1633400c8cca915b92a8d671":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31881affbf374b8d83258dfcaefd2e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3127850acb2341698e1e28d16f387952":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a542cc10e9d94fcb906eb08191c41a08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cab162293054f248580f7dd372c93af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7eec74ff96d6410da31d0d958dfba523":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75036ee99908401d9d48e339da9817ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"333ae03995474910a00ea23908d6b6ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a5b085e68c249d794755a48934e80be","IPY_MODEL_cd155cafd16c41edaf1e25a88cf5cdb8","IPY_MODEL_e010e47a3eca4dd3bd2d371b0f7689a6"],"layout":"IPY_MODEL_643456f8c053447ab1f38a53b8eb193d"}},"2a5b085e68c249d794755a48934e80be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6e6211ba5cc44e9a986e87ff3bde006","placeholder":"​","style":"IPY_MODEL_e431666a9b6c48f9b62afc474de74f1f","value":"Downloading: 100%"}},"cd155cafd16c41edaf1e25a88cf5cdb8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a9e245e573845bc82df498f41ed767b","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db74d85e6bc14c1b8a5b672235342d2f","value":440473133}},"e010e47a3eca4dd3bd2d371b0f7689a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c9bd190b92245bcb7278876e899528e","placeholder":"​","style":"IPY_MODEL_02a2408850bf4f66b356f5134dfe93e8","value":" 420M/420M [00:21&lt;00:00, 34.1MB/s]"}},"643456f8c053447ab1f38a53b8eb193d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6e6211ba5cc44e9a986e87ff3bde006":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e431666a9b6c48f9b62afc474de74f1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a9e245e573845bc82df498f41ed767b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db74d85e6bc14c1b8a5b672235342d2f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1c9bd190b92245bcb7278876e899528e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02a2408850bf4f66b356f5134dfe93e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}